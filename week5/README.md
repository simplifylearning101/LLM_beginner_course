Alright, let's dive into the next topic.

## Week 5: Working with Different Data Modalities 

Up until now, our focus has been primarily on processing **text**, the most common modality for large language models. But the world isn't just words; it's also images, sounds, and videos. This week, we'll expand our capabilities to build applications that can understand and generate content beyond simple text.

We will explore how to interact with **multimodal models** that are trained to process multiple types of data at once. You'll learn the techniques to enable your LLM applications to perform a range of new tasks, including:

* **Image Captioning:** Generating a descriptive text for any given image.
* **Visual Question Answering (VQA):** Answering questions about the content of an image.
* **Audio Transcription:** Converting spoken language from audio files into text.
* **Video Summarization:** Creating a concise text summary of a video's content.

By the end of this week, you'll be able to build truly interactive and versatile agents that can perceive the world in richer ways and respond with greater context. Let's get started with our first modality.

| Hour | Link |
|------|------|
| 1    | [Image Understanding & Generation](01.md) |
| 2    | [Audio Transcription & Summarization](02.md)|
| 3    | [Video Understanding & Summarization](03.md)|
| 4    | [Combining Modalities for Complex Tasks](04.md) |
| 5    | [Real-Time Text-to-Speech & Conversational Agents](05.md) |
| 6    | [Enhanced Multimodal Conversational Bot](06.md) |
| 7    | [](07.md) |
| 8    | [](08.md) |
| 9    | [](09.md) |
| 10   | [](10.md) |
