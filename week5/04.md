### Day 4: Combining Modalities for Complex Tasks

So far, we have processed modalities in isolation. Today, we'll learn how to build a unified system that can process multiple data types in a single query. This is the essence of true **multimodal agents**. We'll use a single API call to handle both an image and a text prompt simultaneously, enabling more complex and contextual conversations.

#### **Hands-on Session: Building a Unified Multimodal Agent**

1.  **Preparation:** We will refactor our code into a more structured way to handle different inputs. Ensure you have the `openai` library and your API key is set up.

2.  **Code: The Multimodal Function:** The key is to dynamically build the `messages` payload for the API call based on the available data. A single function can handle text, image, and potentially other modalities in the future.

    ```python
    import base64
    import openai
    import os

    # Your OpenAI API key
    openai.api_key = os.getenv("OPENAI_API_KEY")

    def encode_image(image_path):
        with open(image_path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode('utf-8')

    def run_multimodal_agent(text_prompt, image_path=None):
        """
        Runs a query with text and an optional image.
        """
        messages_payload = []
        
        # Add the text prompt as the first message
        messages_payload.append({"type": "text", "text": text_prompt})
        
        # If an image path is provided, encode and add it to the payload
        if image_path:
            base64_image = encode_image(image_path)
            messages_payload.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:image/jpeg;base64,{base64_image}"
                }
            })

        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": messages_payload
                    }
                ],
                max_tokens=500
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"An error occurred: {e}"

    # --- Examples of usage ---

    # Example 1: Purely text-based
    text_result = run_multimodal_agent("What is the capital of France?")
    print(f"Text-only query result: {text_result}\n")

    # Example 2: Multimodal (Text + Image)
    image_path = "path/to/your/image.jpg"
    multimodal_result = run_multimodal_agent(
        "Describe the main subject in this image and then answer my question: how would you describe the lighting?",
        image_path=image_path
    )
    print(f"Multimodal query result: {multimodal_result}")
    ```

#### **Homework:**

  * Modify the `run_multimodal_agent` function to also accept an optional `audio_path` argument. Use the logic from Day 2 to transcribe the audio and then include the transcription as part of the text prompt for the model. This will create a truly unified text, image, and audio-aware agent.

-----
