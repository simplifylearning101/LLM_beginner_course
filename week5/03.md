
### Day 3: Video Understanding & Summarization ðŸ“¹

Video is simply a sequence of images and an audio track. By combining the techniques from Day 1 and Day 2, we can build a basic video understanding application. We'll use a library to extract keyframes from a video and then leverage a multimodal model to analyze them.

#### **Hands-on Session: Analyzing a Video**

1.  **Preparation:**

      * You'll need a video file (`.mp4`).
      * Install the `opencv-python` and `moviepy` libraries.

    <!-- end list -->

    ```bash
    pip install opencv-python moviepy
    ```

2.  **Code: Extracting Keyframes and Transcribing Audio**

      * The code below will extract a frame every few seconds from a video and save it to a temporary directory. It will also extract the audio track.

    <!-- end list -->

    ```python
    import cv2
    import os
    import base64
    import openai
    import shutil
    from moviepy.editor import VideoFileClip

    # Your OpenAI API key
    openai.api_key = os.getenv("OPENAI_API_KEY")

    def analyze_video(video_path, seconds_per_frame=5):
        # Create a temp directory to save frames
        frames_dir = "temp_frames"
        if os.path.exists(frames_dir):
            shutil.rmtree(frames_dir)
        os.makedirs(frames_dir)

        # Extract frames
        vidcap = cv2.VideoCapture(video_path)
        fps = vidcap.get(cv2.CAP_PROP_FPS)
        count = 0
        frame_list = []
        while vidcap.isOpened():
            ret, frame = vidcap.read()
            if not ret:
                break
            # Save a frame every `seconds_per_frame` seconds
            if count % int(fps * seconds_per_frame) == 0:
                frame_path = os.path.join(frames_dir, f"frame{int(count / (fps * seconds_per_frame))}.jpg")
                cv2.imwrite(frame_path, frame)
                frame_list.append(frame_path)
            count += 1
        vidcap.release()

        # Encode frames to base64
        base64_frames = [base64.b64encode(open(frame, "rb").read()).decode('utf-8') for frame in frame_list]

        # Extract audio and transcribe
        audio_path = "temp_audio.mp3"
        with VideoFileClip(video_path) as video:
            video.audio.write_audiofile(audio_path, logger=None)
        
        audio_file = open(audio_path, "rb")
        audio_transcript = openai.audio.transcriptions.create(
            model="whisper-1",
            file=audio_file
        )

        # Combine analysis and get a summary
        messages = [
            {"role": "user", "content": "These are frames from a video. Describe the key moments and then summarize the entire video based on both the visuals and the transcript provided below."},
            {"role": "user", "content": f"Audio Transcript: {audio_transcript.text}"},
        ]
        
        # Add the frames to the message payload
        for frame in base64_frames:
            messages[0]["content"] += [{"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{frame}"}}]

        try:
            response = openai.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=500
            )
            print("Video Summary:")
            print(response.choices[0].message.content)
        except Exception as e:
            print(f"An error occurred: {e}")

        # Clean up temporary files
        shutil.rmtree(frames_dir)
        os.remove(audio_path)

    # Path to your video file
    video_path = "path/to/your/video.mp4"
    analyze_video(video_path)
    ```

#### **Homework:**

  * Modify the script to allow a user to ask a specific question about the video, such as "What was the main topic discussed?" or "How did the person in the video react to the event?"
  * Try adjusting the `seconds_per_frame` parameter to see how it affects the quality of the summary. A smaller number will give a more detailed summary but will also be more expensive.

-----

### Advanced & Future Trends in Multimodality ðŸš€

Today, we'll wrap up the week by looking at the cutting edge of multimodal AI. While we've focused on images, audio, and video, the field is rapidly expanding to include other data types and more advanced techniques.

#### **Concepts to Ponder**

  * **Multimodal RAG:** Just as we used text-based RAG to augment our models' knowledge, we can apply the same principle to other modalities. Imagine a database of images, audio files, or videos. We could query it based on a user's text prompt, find the most relevant media, and then provide that to the LLM as context. This is known as **Multimodal Retrieval-Augmented Generation** (Multimodal RAG).
  * **Other Modalities:** Research is ongoing to integrate other data types, such as 3D models, scientific charts and graphs, and even medical imaging like X-rays and MRIs. These specialized models are trained to reason about complex data and can revolutionize fields like healthcare and engineering.
  * **Agentic Frameworks:** The multimodal agent you built in Day 4 is a great starting point. In the future, these agents will likely have a more sophisticated "reasoning loop" that can choose the best modality to answer a question or perform a task. For example, if a user asks for a recipe, the agent might decide to generate an instructional video in addition to providing a text response.

#### **Final Project: The Ultimate Multimodal Assistant**

To solidify your learning, let's build the ultimate project for the week.

1.  **Refine Your Agent:** Use the logic from your Day 4 homework to create a single function that can take any combination of a `text_prompt`, an `image_path`, and an `audio_path`.
2.  **Build a CLI Interface:** Create a command-line interface (CLI) that prompts the user to either enter a text query, provide an image path, or provide an audio path.
3.  **Process and Respond:** Depending on the user's input, call your ultimate multimodal function.
4.  **Display:** Print the final response from the LLM, showing the power of your agent to handle any combination of modalities the user throws at it.

This concludes our focus on multimodal applications. You've taken a significant step toward building truly intelligent and versatile AI systems. 