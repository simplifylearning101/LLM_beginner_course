## Week 4: Building a Full-Stack LLM App

This week's major project will focus on transforming your advanced LLM agent into a complete, user-facing web application. We'll follow a progressive approach, starting with a simple UI and then building out the full-stack version.

### Phase 1: Streamlit Prototype

* **Objective:** Quickly build a functional and simple user interface for your LLM agent.
* **Technology:** We will use **Streamlit** for its ability to create interactive web apps with minimal code, allowing for rapid prototyping and testing of the agent's functionality.
* **Git Integration:** We will initialize a Git repository for the project and make our first commits here.
* **Link:** [Phase 1](phase1.md)


### Phase 2: Full-Stack Development

* **Objective:** Develop a robust and scalable full-stack version of the application.
* **Backend:** We will use **FastAPI** to build a high-performance backend API for the LLM agent. This framework is a great choice for modern Python web applications due to its speed and automatic interactive documentation.
* **Frontend:** The user interface will be built with **ReactJS / NextJS**, which provides a powerful and flexible framework for creating dynamic web applications.
* **Link:** [Phase 2 - Backend Server](phase2_part1.md) > [Phase 2 - Frontend Web App](phase2_part2.md) > [Phase 2 - Integration](phase2_part3.md)

### Phase 3: Cloud Deployment

* **Objective:** Deploy the full-stack application to a cloud service to make it accessible to users.
* **Cloud Provider:** We will choose a cloud provider from **GCP, AWS, or Azure** and use their respective services (e.g., Google App Engine, AWS Elastic Beanstalk, or Azure App Service) to deploy the application, ensuring it is production-ready.
* **Deployment Workflow:** We will integrate Git with the cloud provider's deployment pipeline to enable continuous deployment (CD) for seamless updates.
* **Link:** [GCP Deployment](phase3_gcp.md) > [AWS Deployment](phase3_aws_.md) > [Azure Deployment](phase3_azure.md)