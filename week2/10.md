Absolutely, let's wrap up Week 2 with a powerful final hour\!

-----

### **Week 2: Hour 10 - Advanced Prompting: Chaining, Agentic Patterns & Multimodal Basics**

#### **30% Theory: Orchestrating LLMs & Expanding Capabilities**

  * **Objective:** To introduce the concepts of chaining LLM calls for complex tasks, understanding basic agentic patterns (where LLMs make decisions), and touching upon the emerging power of multimodal LLMs.

  * **Moving Beyond Single-Turn:**

      * So far, most of our tasks involve one LLM call for one job.
      * Real-world applications often need the LLM to perform multiple, sequential steps, sometimes even making choices about which step to take next.

  * **Key Concepts for Advanced LLM Applications:**

    1.  **Chaining LLM Calls:**

          * **What it is:** Using the output of one LLM call as the input for a subsequent LLM call. This allows you to break down complex problems into smaller, manageable steps.
          * **Use Cases:**
              * **Summarize then Translate:** Summarize a document, then translate the summary.
              * **Extract then Generate:** Extract key entities, then generate content based on those entities.
              * **Critique then Refine:** Generate content, then ask the LLM to critique its own output and revise it.
          * **Programmatic Approach:** Store the result of `llm_call_1` in a variable, then pass that variable as part of the prompt for `llm_call_2`.

    2.  **Basic Agentic Patterns (LLM as a "Reasoning Engine"):**

          * **What it is:** Empowering the LLM not just to *generate* text, but to *reason* about a problem, make a decision, and potentially choose which "tool" or next step to take.
          * **Concept of "Tools":** In an agentic setup, the LLM might decide it needs to "search the web" or "query a database." You, the developer, provide the *code* for these tools, and the LLM decides *when* to use them.
          * **Simple Agentic Prompting:** Ask the LLM to output a specific instruction (e.g., "ACTION: SEARCH [query]") which your code then parses and executes.
          * **Why it's powerful:** Automates multi-step workflows.

    3.  **Introduction to Multimodal LLMs (Conceptual):**

          * **What it is:** LLMs that can process and generate content across multiple modalities, not just text. The most common currently are **Vision LLMs** (text + images).
          * **Capabilities:**
              * **Image Captioning:** Describing what's in an image.
              * **Visual Question Answering (VQA):** Answering questions about an image's content.
              * **Image Generation:** (What *you* can do as an AI, with `     `) Generating images from text.
          * **Programmatic Interface:** Typically, you pass image data (e.g., base64 encoded) along with text prompts.
          * **Future:** Audio, video, 3D models.

  * **Connecting to Week 2 Learning:**

      * Chaining relies heavily on your understanding of API calls, structured extraction (for intermediate outputs), and robust error handling (as each step can fail).
      * Agentic patterns leverage strong instruction following and output formatting.

#### **70% Hands-on Session: Chaining LLM Calls & Basic Agentic Logic**

  * **Objective:** To write Python code that demonstrates chaining two LLM calls for a multi-step task and implements a simple agentic pattern where the LLM "decides" on a next step. We'll briefly touch on how a multimodal interaction *would* look conceptually.

  * **Step-by-Step Instructions:**

    1.  **Preparation (5 minutes):**

          * Open VS Code in your `applied_llm_course` folder.
          * Create a new file named `week2_hour10_advanced_llm.py`.
          * Ensure your environment variable for your chosen LLM provider's API key is set.
          * Copy your robust `get_llm_response_robust` helper function from Hour 9 into this file. (This is crucial for production-grade chaining).
          * Add `import json` and `import time` at the top.

    2.  **Basic LLM Setup (10 minutes):**

          * Copy the boilerplate for your chosen provider (import statements, API key retrieval, client initialization) into `week2_hour10_advanced_llm.py`.
          * Ensure `RETRY_EXCEPTIONS` and `LLM_MODEL` are correctly defined as in Hour 9.

    3.  **Task 1: Chaining: Summarize then Translate (20 minutes):**

          * **Goal:** Summarize an article with one LLM call, then take that summary and translate it into another language using a second LLM call.

          * **Modify `week2_hour10_advanced_llm.py`:**

            ```python
            # ... (your robust LLM setup code and get_llm_response_robust function) ...

            print("\n--- Task 1: Chaining - Summarize then Translate ---")

            original_article = """
            Artificial intelligence (AI) has rapidly advanced in recent years, transforming various industries from healthcare to finance. Machine learning, a subset of AI, allows systems to learn from data without being explicitly programmed. Deep learning, in turn, is a specific form of machine learning that uses neural networks with many layers. These technologies are driving innovations like self-driving cars, personalized recommendations, and advanced diagnostics. However, concerns around data privacy, ethical implications, and job displacement also accompany this rapid growth, prompting a global discussion on responsible AI development.
            """
            target_language = "Spanish"

            # --- Step 1: Summarize the article ---
            print("Step 1: Summarizing original article...")
            summarize_prompt = f"""
            Summarize the following article into exactly two concise sentences.
            Focus on the core concepts of AI, machine learning, and deep learning, and mention key benefits and challenges.

            Article:
            ---
            {original_article}
            ---
            """
            messages_summary = []
            system_msg_summary = "You are an expert at concise summarization."
            if 'OpenAI' in globals(): messages_summary.append({"role": "system", "content": system_msg_summary})
            messages_summary.append({"role": "user", "content": summarize_prompt})

            try:
                if 'Anthropic' in globals():
                    summary = get_llm_response_robust(messages_summary, LLM_MODEL, temp=0.3, max_response_tokens=80, system_msg=system_msg_summary)
                else:
                    summary = get_llm_response_robust(messages_summary, LLM_MODEL, temp=0.3, max_response_tokens=80)
                print("Generated Summary:\n", summary)
                time.sleep(1) # Pause for readability
            except Exception as e:
                print(f"Error during summarization step: {e}")
                summary = "Failed to generate summary." # Fallback for next step


            # --- Step 2: Translate the summary ---
            if summary != "Failed to generate summary.":
                print(f"\nStep 2: Translating summary into {target_language}...")
                translate_prompt = f"""
                Translate the following text into {target_language}.
                Ensure the translation is accurate and natural-sounding.

                Text to translate:
                ---
                {summary}
                ---
                """
                messages_translate = []
                system_msg_translate = f"You are a professional translator fluent in {target_language}."
                if 'OpenAI' in globals(): messages_translate.append({"role": "system", "content": system_msg_translate})
                messages_translate.append({"role": "user", "content": translate_prompt})

                try:
                    if 'Anthropic' in globals():
                        translated_text = get_llm_response_robust(messages_translate, LLM_MODEL, temp=0.2, max_response_tokens=100, system_msg=system_msg_translate)
                    else:
                        translated_text = get_llm_response_robust(messages_translate, LLM_MODEL, temp=0.2, max_response_tokens=100)
                    print(f"Translated Text ({target_language}):\n", translated_text)
                except Exception as e:
                    print(f"Error during translation step: {e}")
            ```

          * **Run the script.** Observe the two distinct LLM calls working in sequence.

    4.  **Task 2: Basic Agentic Pattern - LLM Decision Making (20 minutes):**

          * **Goal:** Implement a simple "tool-use" agent where the LLM decides if it needs to use a "search" tool or can answer directly.

          * **Add this as a separate section to your script:**

            ```python
            # ... (previous code including get_llm_response_robust) ...

            print("\n--- Task 2: Basic Agentic Pattern - LLM Decision Making ---")

            # --- Simulate a "tool" (e.g., a web search function) ---
            def simulated_web_search(query):
                print(f"DEBUG: Performing simulated web search for: '{query}'")
                time.sleep(2) # Simulate network delay
                if "Mount Everest" in query:
                    return "Mount Everest, Earth's highest mountain above sea level, is located in the Himalayas. Its summit is 8,848.86 metres (29,031.7 ft) above sea level. It was first successfully summited in 1953 by Tenzing Norgay and Edmund Hillary."
                elif "Amazon River" in query:
                    return "The Amazon River in South America is the largest river by discharge volume of water in the world, and by some definitions, the longest. It flows through Brazil, Peru, and Colombia."
                else:
                    return "No specific information found for that query, but it is a fascinating topic."

            def agent_workflow(user_query):
                print(f"\nUser: {user_query}")
                agent_prompt = f"""
                You are a helpful assistant. Your goal is to answer user questions.
                You have access to a 'SEARCH' tool for factual queries.

                When you need to search for information, your response MUST be in the format:
                ACTION: SEARCH [query_goes_here]

                If you can answer the question directly or after a search, respond with the answer.
                Do NOT output 'ACTION: SEARCH' if you don't need to search or have just completed one.

                User Question: {user_query}
                """
                messages_agent = []
                system_msg_agent = "You are a helpful assistant that can decide to use a SEARCH tool."
                if 'OpenAI' in globals(): messages_agent.append({"role": "system", "content": system_msg_agent})
                messages_agent.append({"role": "user", "content": agent_prompt})

                # First LLM call: decide to answer or search
                try:
                    if 'Anthropic' in globals():
                        llm_decision = get_llm_response_robust(messages_agent, LLM_MODEL, temp=0.0, max_response_tokens=100, system_msg=system_msg_agent)
                    else:
                        llm_decision = get_llm_response_robust(messages_agent, LLM_MODEL, temp=0.0, max_response_tokens=100)

                    if llm_decision.startswith("ACTION: SEARCH"):
                        search_query = llm_decision.replace("ACTION: SEARCH", "").strip()
                        print(f"Agent Decision: Perform search for '{search_query}'")
                        search_results = simulated_web_search(search_query)

                        # Second LLM call: answer based on search results
                        answer_prompt = f"""
                        Based on the following search results, answer the user's original question: '{user_query}'

                        Search Results:
                        ---
                        {search_results}
                        ---
                        """
                        messages_answer = []
                        if 'OpenAI' in globals(): messages_answer.append({"role": "system", "content": system_msg_agent})
                        messages_answer.append({"role": "user", "content": answer_prompt})

                        if 'Anthropic' in globals():
                            final_answer = get_llm_response_robust(messages_answer, LLM_MODEL, temp=0.0, max_response_tokens=150, system_msg=system_msg_agent)
                        else:
                            final_answer = get_llm_response_robust(messages_answer, LLM_MODEL, temp=0.0, max_response_tokens=150)
                        print("Agent Final Answer:\n", final_answer)

                    else:
                        print("Agent Decision: Answer directly.")
                        print("Agent Final Answer:\n", llm_decision)

                except Exception as e:
                    print(f"Error in agent workflow: {e}")
                    print("Agent Fallback: I'm sorry, I couldn't process your request.")

            # Test the agent workflow
            agent_workflow("What is the highest mountain on Earth?")
            agent_workflow("Tell me about the capital of France.") # Should answer directly
            agent_workflow("What is the longest river in South America?")
            ```

          * **Run the script.** Observe how the agent decides whether to "search" or answer directly, and how the search results influence the final answer.

    5.  **Task 3: Conceptual Multimodal Interaction (5 minutes):**

          * **Goal:** Understand how you *would* pass image data if using a multimodal LLM. (No actual LLM call here, as it requires a specific model and API endpoint/data format).

          * **Add this as a separate section to your script:**

            ```python
            # ... (previous code) ...

            print("\n--- Task 3: Conceptual Multimodal Interaction (Vision LLM) ---")

            # In a real multimodal scenario (e.g., GPT-4V, Gemini Pro Vision, Claude 3 Vision),
            # you would typically provide image data along with your text prompt.

            # Example: Imagine you have an image file 'product_image.jpg'
            # You would read it and potentially encode it to base64.
            # (This is illustrative, no actual file reading/encoding for this task)

            # image_path = "product_image.jpg"
            # with open(image_path, "rb") as image_file:
            #     encoded_image = base64.b64encode(image_file.read()).decode('utf-8')

            # The 'messages' structure would then include image content:
            conceptual_multimodal_messages = [
                {"role": "user", "content": [
                    {"type": "text", "text": "Describe this image and tell me the main product shown. Identify any text you see."},
                    # {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{encoded_image}"}} # For OpenAI Vision
                    # Or for Gemini: {"inline_data": {"mime_type": "image/jpeg", "data": encoded_image}}
                    # Or for Claude: This would be a 'source' object in message content
                ]}
            ]

            print("If this were a multimodal LLM, the messages structure would include image data.")
            print("For example, a user message could contain both text and an image reference.")
            print("The LLM would then process both modalities to respond.")
            # print(conceptual_multimodal_messages) # Uncomment to see the conceptual structure
            ```

          * **Run the script.** This is a conceptual exercise, so it will simply print explanatory text.

  * **Conclusion of Hands-on Session:**

      * You've completed Week 2 by tackling advanced LLM patterns\! You've successfully implemented chaining LLM calls for multi-step workflows, and built a simple agent that makes decisions. You also have a conceptual understanding of multimodal interactions. These skills are fundamental for building sophisticated and intelligent applications with LLMs.

-----

### **Homework for Hour 10**

  * **Exercise 1: Chained Content Generation & Critique:**
      * Create a new Python file `week2_hour10_homework_critique.py`.
      * **Part A: Initial Generation:**
          * Prompt your chosen LLM to "Generate a short (2-3 sentence) marketing slogan and description for a new AI-powered personal assistant device." Use a `temperature` of 0.7.
      * **Part B: Self-Critique & Refinement:**
          * Take the output from Part A. In a *second* LLM call, prompt the LLM to "Review the following marketing slogan and description. Identify one area for improvement (e.g., clarity, excitement, conciseness). Then, provide a revised slogan and description based on your critique."
      * **Submit:** Your `week2_hour10_homework_critique.py` script and the output from both generation steps (initial, critique, and revised).
  * **Exercise 2: Advanced Agentic Decision:**
      * Enhance the `agent_workflow` from Task 2.
      * Modify the initial prompt to the LLM so it can decide between:
        1.  `ACTION: SEARCH [query]` (as implemented)
        2.  `ACTION: SUMMARIZE [text_to_summarize]` (if the user asks to summarize text provided in the prompt)
        3.  `ACTION: TRANSLATE [text_to_translate] [target_language]` (if the user asks to translate provided text)
      * Implement simple Python functions for `simulated_summarize(text)` and `simulated_translate(text, lang)` (you can just return a placeholder string for now, or use `get_llm_response_robust` to actually perform these actions).
      * Test the agent with queries that trigger each of the three actions.
      * **Submit:** Your updated `week2_hour10_homework_critique.py` (or a new file) script and the output from testing the three action types.
  * **Exercise 3: Multimodal Use Cases:**
      * Brainstorm two distinct real-world applications where a **multimodal LLM (specifically text + image)** would be significantly more effective than a text-only LLM.
      * For each application, describe:
        1.  The problem it solves.
        2.  How the text input would be used.
        3.  How the image input would be used.
        4.  What kind of output you would expect from the LLM.

-----

Congratulations on completing Week 2 of Applied LLM Development\! You've moved from basic API calls to building robust, conversational, and even agentic LLM applications. You've also gained a conceptual understanding of multimodal capabilities. This is a very strong foundation for the more complex topics in Week 3. Take a well-deserved break, and then get ready to dive even deeper\!