Excellent\! You've successfully made your first programmatic LLM calls, which is a huge step. Now, in **Week 2: Hour 2**, we're going to dive deeper into the structure of API calls, focusing on how we pass prompts, control output, and understand the basic structure of the responses we get back. This is key for robust prompt engineering and application building.

-----

### **Week 2: Hour 2 - Deeper into API Calls: Prompt Structure & Response Parsing**

#### **30% Theory: Understanding the Conversation & Controlling Output**

  * **Objective:** To understand how LLM APIs handle multi-turn conversations, how to pass different prompt "roles," and how to access specific parts of the LLM's response programmatically.

  * **The `messages` Array: LLM Conversation History:**

      * When we use an LLM API (especially "chat completion" endpoints like OpenAI's `gpt-3.5-turbo`, Google's `gemini-pro`, or Anthropic's `claude-3-opus`), we don't just send a single string prompt.
      * Instead, we send a list (or array) of "message objects." Each object has a `role` and `content`.
      * **Roles:**
          * **`user`:** Your prompts, questions, or instructions.
          * **`assistant`:** The LLM's previous responses in the conversation.
          * **`system` (OpenAI/Claude):** A special initial message that sets the overall behavior, persona, or rules for the LLM *for the entire conversation*. This is extremely powerful for prompt engineering. (Google Gemini often incorporates system-like instructions directly in the first `user` turn or a dedicated safety setting).
      * **Why this structure?** This `messages` array is how the LLM maintains "memory" of the conversation. Every time you make a new request, you send the *entire history* (or a truncated version) to give the LLM full context.

  * **Controlling Output with API Parameters:**

      * Beyond the `prompt` itself, API calls allow us to fine-tune LLM behavior:
          * **`model`:** Specifies which LLM model you want to use (e.g., `gpt-4`, `gemini-1.5-pro`, `claude-3-sonnet-20240229`). Different models have different capabilities and costs.
          * **`max_tokens`:** Sets the maximum length of the LLM's generated response. Crucial for cost control and preventing overly verbose answers.
          * **`temperature`:** (As discussed in Week 1) Controls the randomness/creativity of the output (0.0 for deterministic, higher for more creative).
          * **`top_p`:** Another way to control randomness (related to `temperature`). You usually use one or the other.
          * **`stop_sequences`:** Specific strings where the LLM should stop generating text (e.g., `\n\nHuman:` to prevent it from role-playing).

  * **Parsing the Response Object:**

      * The LLM API doesn't just send back a string. It sends a structured JSON object (or a Python object that acts like one) containing lots of information:
          * The actual generated text.
          * Metadata about the request (e.g., model used, timestamps).
          * Usage statistics (how many tokens were used, for billing).
      * Knowing how to navigate this object (`response.choices[0].message.content` for OpenAI, `response.text` for Gemini, `message.content[0].text` for Anthropic) is essential for extracting the LLM's output.

#### **70% Hands-on Session: Multi-turn Conversations & Parameter Tuning**

  * **Objective:** To write Python code that simulates a multi-turn conversation, utilizes different `roles` (especially `system` messages), and experiments with `max_tokens` and `temperature`. We'll stick to *one provider* for this session to keep it focused.

  * **Step-by-Step Instructions:**

    1.  **Preparation (5 minutes):**

          * Open VS Code in your `applied_llm_course` folder.
          * Create a new file named `week2_hour2_conversation.py`.
          * Ensure your environment variable for your chosen LLM provider's API key (e.g., `OPENAI_API_KEY`, `GOOGLE_API_KEY`, or `ANTHROPIC_API_KEY`) is set in your VS Code terminal.

    2.  **Basic Setup (10 minutes):**

          * Copy the boilerplate for your chosen provider from `week2_hour1_llm_call.py` (import statements, API key retrieval, client initialization) into `week2_hour2_conversation.py`.

    3.  **Task 1: The System Message - Setting Persona (OpenAI/Anthropic) / Initial Instructions (Gemini) (15 minutes):**

          * **Goal:** Use the `system` role (or initial setup for Gemini) to establish a persistent persona or set of rules for the LLM.

          * **OpenAI Example:**

            ```python
            # ... (your existing setup code) ...

            # Define a list to hold our conversation messages
            messages = [
                {"role": "system", "content": "You are a friendly and enthusiastic travel agent, always eager to help plan amazing trips. Your tone is always positive."},
                {"role": "user", "content": "I'm thinking of a vacation. Where should I go?"}
            ]

            print(f"Sending initial prompt (with system message) to OpenAI LLM...")

            try:
                response = client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=messages,
                    max_tokens=100, # Allow for a slightly longer, enthusiastic response
                    temperature=0.7 # A bit more creative/enthusiastic
                )
                assistant_response = response.choices[0].message.content
                print("\nAssistant (Travel Agent):")
                print(assistant_response)

                # Add assistant's response to the history for the next turn
                messages.append({"role": "assistant", "content": assistant_response})

            except Exception as e:
                print(f"An error occurred: {e}")
            ```

          * **Google Gemini Example:** (Gemini often integrates system instructions into the first user message or through safety settings, but for simpler persona, direct instruction works well)

            ```python
            # ... (your existing setup code) ...

            model = genai.GenerativeModel('gemini-pro')

            # Gemini often uses a list of dictionaries for conversation history,
            # but for initial persona, a strong first user message works.
            # We'll build a messages list similar to OpenAI for consistency in later tasks.
            messages = [
                {"role": "user", "parts": ["You are a friendly and enthusiastic travel agent, always eager to help plan amazing trips. Your tone is always positive. I'm thinking of a vacation. Where should I go?"]}
            ]

            print(f"Sending initial prompt to Google Gemini LLM...")

            try:
                # For Gemini, subsequent calls in a conversation use model.start_chat()
                # For this first turn, we generate content.
                response = model.generate_content(
                    messages,
                    generation_config=genai.types.GenerationConfig(temperature=0.7, max_output_tokens=100)
                )
                assistant_response = response.text
                print("\nAssistant (Travel Agent):")
                print(assistant_response)

                # Add assistant's response to the history for the next turn
                messages.append({"role": "model", "parts": [assistant_response]}) # Gemini uses 'model' role

            except Exception as e:
                print(f"An error occurred: {e}")
            ```

          * **Anthropic Claude Example:**

            ```python
            # ... (your existing setup code) ...

            messages = [
                {"role": "user", "content": "You are a friendly and enthusiastic travel agent, always eager to help plan amazing trips. Your tone is always positive. I'm thinking of a vacation. Where should I go?"}
                # Claude 3 models don't have a distinct "system" role in the messages list for general persona,
                # but you can prefix the first user message for strong persona setting.
                # For dedicated system prompts, it's a separate parameter in client.messages.create() (system="...")
                # For simplicity here, we embed it in the user message for a first turn.
            ]

            print(f"Sending initial prompt to Anthropic Claude LLM...")

            try:
                message = client.messages.create(
                    model="claude-3-opus-20240229", # Or a cheaper model like sonnet/haiku
                    max_tokens=100,
                    temperature=0.7,
                    messages=messages, # Pass the user message
                    system="You are a friendly and enthusiastic travel agent, always eager to help plan amazing trips. Your tone is always positive." # Use dedicated system parameter for Claude 3
                )
                assistant_response = message.content[0].text
                print("\nAssistant (Travel Agent):")
                print(assistant_response)

                # For subsequent turns in Claude, you'd add both user and assistant to the messages list
                # For now, let's keep it simple for the first turn and just get the response.
                messages.append({"role": "assistant", "content": assistant_response}) # Add assistant's response

            except Exception as e:
                print(f"An error occurred: {e}")
            ```

          * **Run your chosen example.** Observe the travel agent persona.

    4.  **Task 2: Multi-Turn Conversation (Continuing the example, 15 minutes):**

          * **Goal:** Implement a follow-up interaction, explicitly adding both your prompt and the LLM's previous response to the `messages` list.

          * **Continue your chosen example's code:**

            ```python
            # ... (code from Task 1, after adding assistant_response to messages list) ...

            # User's second turn
            user_turn_2 = "That sounds exciting! What kind of activities can I do in [LLM's suggested location]?" # Replace with actual location from LLM's response
            messages.append({"role": "user", "content": user_turn_2}) # Add user's new message

            print(f"\nUser's second turn: '{user_turn_2}'")
            print("Sending next turn to LLM (with full history)...")

            try:
                if 'OpenAI' in globals(): # Check which client is initialized
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=messages,
                        max_tokens=150, # Allow for more detail
                        temperature=0.7
                    )
                    assistant_response_2 = response.choices[0].message.content
                elif 'genai' in globals():
                    # For Gemini, subsequent calls in a conversation usually use chat.send_message
                    # For this example, we'll recreate the model.generate_content for simplicity to match the structure.
                    # In a real app, you'd use a chat session object.
                    response = model.generate_content(
                        messages,
                        generation_config=genai.types.GenerationConfig(temperature=0.7, max_output_tokens=150)
                    )
                    assistant_response_2 = response.text
                elif 'Anthropic' in globals():
                    message = client.messages.create(
                        model="claude-3-opus-20240229",
                        max_tokens=150,
                        temperature=0.7,
                        messages=messages, # Pass the updated messages list
                        system="You are a friendly and enthusiastic travel agent, always eager to help plan amazing trips. Your tone is always positive."
                    )
                    assistant_response_2 = message.content[0].text

                print("\nAssistant (Travel Agent) Second Response:")
                print(assistant_response_2)

            except Exception as e:
                print(f"An error occurred in second turn: {e}")
            ```

          * **Run this extended code.** Observe how the LLM maintains the persona and context.

    5.  **Task 3: Experimenting with `temperature` and `max_tokens` (15 minutes):**

          * **Goal:** See how these parameters directly affect the output.
          * **Modify your code:**
              * **Change `temperature`:** Rerun your code from Task 1 (the initial prompt) with `temperature=0.1` (very low, deterministic) and then `temperature=1.0` (high, creative). Compare the tone and uniqueness of responses.
              * **Change `max_tokens`:** In the Task 2 code, set `max_tokens=20` and rerun. Notice how the LLM's response gets cut off. Then set it to `max_tokens=300` and see if the response is longer.
          * **Observe:** Note the direct impact of these parameters.

    6.  **Task 4: Parsing the Full Response Object (10 minutes):**

          * **Goal:** Understand that there's more than just the content in the response.

          * **Add these lines to your code (after getting `llm_response_content` or `assistant_response`):**

            ```python
            # Print the full response object to see all its details
            # For OpenAI, it's 'response'
            # For Gemini, it's 'response'
            # For Anthropic, it's 'message'
            # (You might need to adjust based on your chosen client)
            print("\n--- Full Response Object (for debugging/info) ---")
            # For OpenAI:
            # print(response)
            # For Google Gemini:
            # print(response)
            # For Anthropic:
            # print(message)

            # More specifically, let's try to print token usage for OpenAI
            if 'OpenAI' in globals() and response.usage:
                print(f"Tokens Used (OpenAI): Input={response.usage.prompt_tokens}, Output={response.usage.completion_tokens}, Total={response.usage.total_tokens}")
            ```

          * **Run the code.** Examine the output. You'll see metadata like token counts, model IDs, etc. This information is vital for cost tracking and performance analysis later.

  * **Conclusion of Hands-on Session:**

      * You've taken a significant step in understanding the mechanics of LLM API calls. By controlling the conversation history with `messages` roles, tuning `temperature` and `max_tokens`, and inspecting the full response object, you now have programmatic control over LLM interactions. This is the bedrock of building sophisticated LLM applications.

-----

### **Homework for Hour 2**

  * **Exercise 1: Personalized Assistant (for your chosen provider):**
      * Create a new Python file named `week2_hour2_homework.py`.
      * Write a script that initializes an LLM as a "helpful coding assistant" (using a `system` message for OpenAI/Anthropic, or strong initial prompt for Gemini).
      * Have the assistant respond to *two* user queries:
        1.  "How do I sort a list in Python?"
        2.  "What is the best way to handle errors in Python?"
      * Ensure the assistant maintains its "coding assistant" persona throughout. Limit each response to `max_tokens=100`.
      * **Submit:** Your `week2_hour2_homework.py` script and the full conversational output.
  * **Exercise 2: Temperature Comparison:**
      * Using your script from Exercise 1, change the `temperature` parameter to `0.1` and run it.
      * Then, change the `temperature` parameter to `0.9` and run it again.
      * **Submit:** A brief written comparison (2-3 sentences) of the outputs for the two temperature settings. How did the responses differ in creativity, tone, or directness?
  * **Exercise 3: Inspecting the Response (Token Counts):**
      * In your `week2_hour2_homework.py` script (from Exercise 1), add code to print the token usage (prompt tokens, completion tokens, total tokens) after *each* LLM response.
      * **Submit:** The script output showing the token counts for both turns of the conversation. Why might these numbers be important for a real-world application? (1-2 sentences).

-----

This hour really elevates your understanding of how to communicate effectively and programmatically with LLMs. You're now ready to start thinking about more complex prompting techniques in code\!