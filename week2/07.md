Fantastic work with content generation\! You're becoming proficient at guiding LLMs programmatically. For **Week 2: Hour 7**, we're going to explore a crucial aspect of building interactive LLM applications: **Chatbots and Conversational Agents**. This moves beyond single-turn requests to creating persistent, stateful interactions.

-----

### **Week 2: Hour 7 - Building Conversational Agents & Chatbots**

#### **30% Theory: State, Context, and Continuous Conversation**

  * **Objective:** To understand the fundamental difference between single-turn LLM calls and multi-turn conversations, and to learn how to maintain conversation state and manage context in your Python applications.

  * **The Illusion of LLM "Memory":**

      * As we touched on in Week 1, LLMs don't inherently "remember" past turns in a conversation. Each API call is, in essence, a fresh request.
      * The "memory" is an illusion created by you, the developer, by sending the *entire conversation history* with each new prompt.

  * **Key Concepts for Conversational Agents:**

    1.  **Conversation State:**

          * **What it is:** The complete history of messages exchanged between the user and the LLM.
          * **In code:** Typically maintained as a Python list of message dictionaries (`[{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...]`).
          * **Management:** With each new user input, you append the user's message to this list, then send the *entire updated list* to the LLM. When the LLM responds, you append its response to the same list.

    2.  **Context Window Limits:**

          * **The Challenge:** LLMs have a maximum "context window" (a limit on how many tokens - input + output - they can process in a single request). If your conversation history gets too long, you'll hit this limit, leading to errors or truncation.
          * **Strategies:**
              * **Truncation:** Removing older messages from the beginning of the `messages` list when it approaches the limit. (Simplest, but loses oldest context).
              * **Summarization:** Periodically summarizing old parts of the conversation and replacing multiple old messages with a single summary message. (More complex, but preserves crucial context).
              * **Retrieval Augmented Generation (RAG):** (Advanced, we'll cover later) Fetching relevant information from an external knowledge base based on the current conversation, rather than relying solely on chat history.

    3.  **System/Initial Instructions for Chatbots:**

          * **What it is:** The initial prompt that defines the chatbot's persona, rules, and overarching goals. This is critical for consistent behavior.
          * **In code:** Use the `system` role (OpenAI/Anthropic) as the very first message in your `messages` list. For Gemini, integrate these instructions into the initial user prompt or use their safety/configuration settings.

    4.  **User Interface (Simulated):**

          * For our hands-on, we'll use simple `input()` and `print()` in the terminal to simulate a chatbot interface. In a real application, this would be a web UI, mobile app, etc.

#### **70% Hands-on Session: Building a Simple Conversational Chatbot**

  * **Objective:** To write a Python script that simulates a multi-turn chatbot, maintaining conversation history, defining a persona, and implementing a basic context window management strategy. We'll stick to your chosen provider for clarity.

  * **Step-by-Step Instructions:**

    1.  **Preparation (5 minutes):**

          * Open VS Code in your `applied_llm_course` folder.
          * Create a new file named `week2_hour7_chatbot.py`.
          * Ensure your environment variable for your chosen LLM provider's API key is set in your VS Code terminal.

    2.  **Basic Setup (10 minutes):**

          * Copy the boilerplate for your chosen provider (import statements, API key retrieval, client initialization) into `week2_hour7_chatbot.py`.
          * Add `import time` at the top. We'll use this to pause slightly for a more natural chat experience.

    3.  **Task 1: Initializing the Chatbot with Persona (15 minutes):**

          * **Goal:** Set up the chatbot's initial identity and start the conversation history.

          * **Modify `week2_hour7_chatbot.py`:**

            ```python
            # ... (your existing setup code for your chosen provider) ...
            import time # Add this line at the top

            print("\n--- Simple Chatbot Example ---")
            print("Type 'quit' or 'exit' to end the conversation.")

            # Initialize conversation history
            # This list will hold all messages (user and assistant)
            conversation_history = []
            MAX_CONVERSATION_TOKENS = 500 # A soft limit to manage context window

            # Define the chatbot's persona/system instruction
            if 'OpenAI' in globals():
                system_instruction_content = "You are a friendly and knowledgeable AI assistant specialized in science facts. Your responses should be concise, accurate, and engaging for a general audience. Never answer non-science related questions."
                conversation_history.append({"role": "system", "content": system_instruction_content})
                llm_model = "gpt-3.5-turbo"
            elif 'genai' in globals():
                # For Gemini, system instructions are often embedded in the first user message or use safety settings.
                # We'll embed a strong persona in the first prompt.
                llm_model = 'gemini-pro'
                # For Gemini, the actual chat session will handle context. We'll start it directly.
            elif 'Anthropic' in globals():
                system_instruction_content = "You are a friendly and knowledgeable AI assistant specialized in science facts. Your responses should be concise, accurate, and engaging for a general audience. Never answer non-science related questions."
                # For Claude 3, the system parameter is separate from messages for clarity
                llm_model = "claude-3-opus-20240229" # Or a cheaper model
            else:
                print("Error: No LLM provider initialized. Please choose one.")
                exit()


            # For Gemini, we need to initialize a chat session
            if 'genai' in globals():
                chat_session = model.start_chat(history=[]) # Start with an empty history
                # We'll add the persona instruction to the very first user message for Gemini
                print("Gemini Chatbot initialized. Please start by asking a science fact.")


            def get_llm_response(current_messages, max_response_tokens=100):
                """Helper function to get response from the chosen LLM."""
                try:
                    if 'OpenAI' in globals():
                        response = client.chat.completions.create(
                            model=llm_model,
                            messages=current_messages,
                            max_tokens=max_response_tokens,
                            temperature=0.7 # Keep it engaging
                        )
                        return response.choices[0].message.content
                    elif 'genai' in globals():
                        # For Gemini, the chat_session manages the history automatically
                        # We send only the new message, the session keeps track.
                        # However, for this simplified example using `generate_content` for flexibility,
                        # we'll pass the full `current_messages` list each time.
                        # In a true interactive Gemini chat, you'd use `chat_session.send_message(user_message_content)`
                        response = model.generate_content(
                            current_messages,
                            generation_config=genai.types.GenerationConfig(temperature=0.7, max_output_tokens=max_response_tokens)
                        )
                        return response.text
                    elif 'Anthropic' in globals():
                        message = client.messages.create(
                            model=llm_model,
                            max_tokens=max_response_tokens,
                            temperature=0.7,
                            messages=current_messages[1:] if current_messages[0]['role'] == 'system' else current_messages, # Remove system for messages list
                            system=system_instruction_content if current_messages[0]['role'] == 'system' else None
                        )
                        return message.content[0].text
                except Exception as e:
                    print(f"Error fetching LLM response: {e}")
                    return "I'm having trouble responding right now. Please try again."

            ```

          * **Run the script.** The chatbot won't do much yet, but you've set up the core `conversation_history` and `get_llm_response` helper.

    4.  **Task 2: Implementing the Chat Loop (20 minutes):**

          * **Goal:** Create an infinite loop that takes user input, sends it to the LLM (with history), and prints the LLM's response.

          * **Add the following to the end of your script:**

            ```python
            # ... (code from Task 1, including get_llm_response function) ...

            print("\nScience Bot: Hello! I'm your science fact assistant. Ask me anything about science!")
            # For Gemini, the persona is also delivered with the first actual interaction
            if 'genai' in globals():
                 conversation_history.append({"role": "model", "parts": ["Hello! I'm your science fact assistant. Ask me anything about science!"]})


            while True:
                user_input = input("\nYou: ").strip()
                if user_input.lower() in ["quit", "exit"]:
                    print("Science Bot: Goodbye! Stay curious!")
                    break

                print("Science Bot: Thinking...")
                time.sleep(1) # Simulate thinking time

                # Add user message to history
                if 'genai' in globals():
                    # For Gemini, we might manage history slightly differently if using chat_session.
                    # For `generate_content` we keep building the list.
                    conversation_history.append({"role": "user", "parts": [user_input]})
                else: # OpenAI and Anthropic
                    conversation_history.append({"role": "user", "content": user_input})


                # Get response
                llm_response = get_llm_response(conversation_history)
                print(f"Science Bot: {llm_response}")

                # Add assistant response to history
                if 'genai' in globals():
                     conversation_history.append({"role": "model", "parts": [llm_response]})
                else: # OpenAI and Anthropic
                    conversation_history.append({"role": "assistant", "content": llm_response})

                # (Optional) Print current history length
                # print(f"Current history length: {len(conversation_history)} messages")
            ```

          * **Run the script.** Interact with your chatbot\! Ask a few science questions, then ask a non-science question to see if its persona holds.

    5.  **Task 3: Basic Context Window Management (Truncation) (15 minutes):**

          * **Goal:** Implement a simple strategy to prevent the conversation history from growing indefinitely and hitting context limits.

          * **Modify the `while` loop to add context management:**

            ```python
            # ... (previous code) ...

            # Before calling get_llm_response inside the loop:
            # Simple context window management: remove oldest messages if too long
            while calculate_token_length(conversation_history) > MAX_CONVERSATION_TOKENS:
                if len(conversation_history) <= 1: # Can't remove if only system/first prompt
                    break
                # Remove the oldest non-system/non-initial assistant message
                # For OpenAI/Anthropic, always preserve the initial system message if it exists
                # For Gemini, remove the oldest user/model pair.
                if conversation_history[0].get('role') == 'system':
                    # Keep system message, remove next oldest pair
                    if len(conversation_history) >= 3: # Need at least system + user + assistant
                        conversation_history.pop(1) # Remove oldest user
                        conversation_history.pop(1) # Remove oldest assistant (now at index 1)
                elif len(conversation_history) >= 2: # No system message, remove oldest user/model pair
                    conversation_history.pop(0) # Remove oldest user
                    conversation_history.pop(0) # Remove oldest assistant (now at index 0)
                else:
                    break # Not enough messages to remove safely

                print("--- (Old messages truncated to manage context) ---")

            # ... (rest of the while loop) ...


            # Add a helper function to estimate token length (simple char count for now, proper tokenization later)
            def calculate_token_length(messages_list):
                total_chars = 0
                for message in messages_list:
                    if 'content' in message: # OpenAI, Anthropic
                        total_chars += len(message['content'])
                    elif 'parts' in message: # Gemini
                        for part in message['parts']:
                            if isinstance(part, str):
                                total_chars += len(part)
                # This is a very rough estimate. Actual tokens are model-specific.
                # A common rule of thumb is 1 token ~ 4 characters.
                return total_chars / 4


            # Modify the initial print statement for Gemini if system message is not explicit
            if 'genai' in globals():
                print("Science Bot: Hello! I'm your science fact assistant. Ask me anything about science!")
                # For Gemini, the first actual interaction often also sets the initial tone/persona
                # The `conversation_history` will explicitly store the user-model turns.
                # Ensure MAX_CONVERSATION_TOKENS is reasonable, e.g., 500 for a few turns.
            ```

          * **Run the script.** Engage in a longer conversation (more than 5-6 turns). You should eventually see the "Old messages truncated..." message, indicating your context management is working. (Note: `MAX_CONVERSATION_TOKENS` is a *very rough* character-based estimate here; proper token counting requires a library, which we'll cover later).

  * **Conclusion of Hands-on Session:**

      * You've built a functional, multi-turn chatbot in Python\! You now understand how to maintain conversation history, enforce a persona, and implement a basic strategy for managing the LLM's context window. These are essential skills for creating any interactive LLM application.

-----

### **Homework for Hour 7**

  * **Exercise 1: Extend Your Chatbot's Persona:**
      * In your `week2_hour7_chatbot.py` script:
          * Change the chatbot's persona from "science facts assistant" to something else (e.g., "a helpful historical guide," "a sarcastic movie critic," "a friendly coding tutor").
          * Modify the `system` message (or initial Gemini instruction) to reflect this new persona.
          * Add *one* specific rule to its behavior (e.g., "Always respond with a question at the end," "Never give direct answers, only hints," "Always recommend a movie from the 80s if asked for a recommendation").
      * Test it with a few turns.
      * **Submit:** Your modified `week2_hour7_chatbot.py` script and a screenshot or text log of a 3-4 turn conversation demonstrating the new persona and rule.
  * **Exercise 2: Improved Context Management Idea:**
      * The current token counting in `calculate_token_length` is very basic. What is a more accurate way to count tokens for OpenAI, Gemini, or Anthropic models? (Hint: The API libraries often have utilities or the tokenizers are publicly available).
      * Describe briefly (2-3 sentences) how you *could* implement a more accurate token counting method if you were to improve the `calculate_token_length` function.
  * **Exercise 3: Chatbot Limitations:**
      * Ask your chatbot (with its new persona from Exercise 1) two questions that are *outside* its defined scope or require complex reasoning it might struggle with.
      * **Submit:** The two questions you asked and the chatbot's responses. Briefly analyze (1-2 sentences) why the chatbot struggled or how it tried to adhere to its rules.

-----

You've just built your first interactive LLM application\! This is a significant milestone. Understanding conversational flow and context management is fundamental for any advanced LLM development. Keep up the fantastic work\!