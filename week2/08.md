Alright, you're now capable of building conversational agents and managing context, which is fantastic\! For **Week 2: Hour 8**, we're going to dive into a critical practical aspect: **Handling Input & Output for Real-World Applications**. This moves beyond simple `input()`/`print()` to thinking about how LLMs integrate into larger systems.

-----

### **Week 2: Hour 8 - Handling Input & Output: Files, Web & User Interfaces**

#### **30% Theory: Beyond the Terminal - Integrating LLMs**

  * **Objective:** To understand how LLMs interact with various data sources (files, web content) and different user interfaces, moving our thinking from simple terminal scripts to more robust application architectures.

  * **Why Input/Output is Crucial:**

      * Real-world LLM applications don't just take typed prompts. They process documents, web pages, database entries, user voice input, etc.
      * Similarly, their output isn't always just printed to the console; it might update a database, send an email, generate a report, or populate a web page.

  * **Common Input Sources for LLMs:**

    1.  **Local Files:**

          * **Text files (`.txt`, `.md`):** Simple reading.
          * **Structured files (`.json`, `.csv`, `.xml`):** Read, parse, extract relevant text, then send to LLM.
          * **Documents (`.pdf`, `.docx`):** Requires libraries to extract text content before LLM processing.
          * **Images (`.jpg`, `.png`):** Requires Vision LLMs (like GPT-4V, Gemini Pro Vision) or OCR (Optical Character Recognition) to extract text, or description generation for text-only LLMs. (Beyond scope for this hour but good to know).

    2.  **Web Content:**

          * **URLs/Web Pages:** Fetching HTML content, then parsing it to extract relevant text (e.g., using `BeautifulSoup`).
          * **APIs (other APIs):** Calling other web services to retrieve data (e.g., weather APIs, news APIs), then passing that data to the LLM.

    3.  **User Interfaces:**

          * **Web Forms:** Text input from a web page.
          * **Mobile Apps:** Text input from a mobile UI.
          * **Speech-to-Text (STT):** Transcribing spoken words into text for the LLM.

  * **Common Output Destinations for LLMs:**

    1.  **Local Files:**
          * Saving generated content, summaries, or extracted data to a file.
    2.  **Web/Application UI:**
          * Displaying LLM responses in a web page, mobile app, or desktop application.
    3.  **Databases:**
          * Storing structured extracted data or generated content.
    4.  **APIs (other APIs):**
          * Sending LLM-generated content to another service (e.g., a mailing service, a publishing platform).
    5.  **Text-to-Speech (TTS):**
          * Converting LLM's text response into spoken audio.

  * **Key Python Libraries:**

      * **File I/O:** Built-in Python `open()`.
      * **Web Requests:** `requests` for fetching URLs.
      * **HTML Parsing:** `BeautifulSoup` for extracting text from HTML.
      * **JSON/CSV:** Built-in `json` and `csv` modules.

#### **70% Hands-on Session: Reading from Files & Web, Writing to Files**

  * **Objective:** To write Python code that demonstrates reading text content from a local file and a public web page, sending that content to an LLM for processing, and then saving the LLM's response to a new file. We'll use your chosen provider.

  * **Step-by-Step Instructions:**

    1.  **Preparation (5 minutes):**

          * Open VS Code in your `applied_llm_course` folder.
          * Create a new file named `week2_hour8_io_llm.py`.
          * Ensure your environment variable for your chosen LLM provider's API key is set in your VS Code terminal.
          * **Install `requests` and `BeautifulSoup4`:** In your VS Code terminal, type:
            ```bash
            pip install requests beautifulsoup4
            ```

    2.  **Basic LLM Setup (10 minutes):**

          * Copy the boilerplate for your chosen provider (import statements, API key retrieval, client initialization) into `week2_hour8_io_llm.py`.
          * Add `import os`, `import requests`, `from bs4 import BeautifulSoup` at the top.

    3.  **Task 1: Reading from a Local Text File & Summarizing (20 minutes):**

          * **Goal:** Read the content of a `.txt` file, summarize it with an LLM, and print the summary.

          * **Create a sample text file:**

              * In your `applied_llm_course` folder, create a new file named `sample_article.txt`.
              * Paste the following text into `sample_article.txt` and save it:
                ```text
                Mars, often called the "Red Planet," is the fourth planet from the Sun and the second-smallest planet in the Solar System, after Mercury. In English, Mars carries the name of the Roman god of war and is often referred to as the "Red Planet" because of the reddish-orange appearance of its surface, which is caused by iron oxide. Mars is a terrestrial planet with a thin atmosphere, primarily carbon dioxide. Its surface is scarred by numerous impact craters, as well as distinct volcanoes and canyons. It has two small and irregularly shaped moons, Phobos and Deimos. Scientists are particularly interested in Mars because of evidence that it once had liquid water on its surface, and thus the potential for past or present life. Missions like NASA's Perseverance rover are actively exploring its geology and searching for biosignatures.
                ```

          * **Modify `week2_hour8_io_llm.py`:**

            ```python
            # ... (your existing LLM setup code) ...
            import os
            import requests
            from bs4 import BeautifulSoup

            # --- Helper function for LLM interaction (reusing from Hour 7) ---
            def get_llm_response(messages_list, model_name, temp=0.5, max_response_tokens=150, system_msg=None, json_format=False):
                try:
                    if 'OpenAI' in globals():
                        response = client.chat.completions.create(
                            model=model_name,
                            messages=messages_list,
                            max_tokens=max_response_tokens,
                            temperature=temp,
                            response_format={"type": "json_object"} if json_format else {"type": "text"}
                        )
                        return response.choices[0].message.content
                    elif 'genai' in globals():
                        response = model.generate_content(
                            messages_list,
                            generation_config=genai.types.GenerationConfig(temperature=temp, max_output_tokens=max_response_tokens)
                        )
                        return response.text
                    elif 'Anthropic' in globals():
                        message = client.messages.create(
                            model=model_name,
                            max_tokens=max_response_tokens,
                            temperature=temp,
                            messages=messages_list[1:] if messages_list and messages_list[0]['role'] == 'system' else messages_list,
                            system=system_msg if system_msg else None
                        )
                        return message.content[0].text
                except Exception as e:
                    print(f"Error fetching LLM response: {e}")
                    return "Error processing request."

            # --- Task 1: Read from local file & Summarize ---
            print("\n--- Task 1: Summarizing from Local File ---")
            file_path = "sample_article.txt"
            article_content = ""
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    article_content = f.read()
                print(f"Successfully read {len(article_content)} characters from {file_path}")
            except FileNotFoundError:
                print(f"Error: File '{file_path}' not found. Please create it.")
                exit()

            # Construct prompt for summarization
            summarize_prompt = f"""
            Summarize the following article about Mars into three concise sentences.
            Focus on its key characteristics and why scientists are interested in it.

            Article:
            ---
            {article_content}
            ---
            """
            llm_model_name = "gpt-3.5-turbo" # Adjust for your chosen model if needed
            if 'genai' in globals(): llm_model_name = 'gemini-pro'
            if 'Anthropic' in globals(): llm_model_name = "claude-3-opus-20240229"

            messages_task1 = []
            system_msg_task1 = "You are a concise summarizer."
            if 'OpenAI' in globals(): messages_task1.append({"role": "system", "content": system_msg_task1})
            messages_task1.append({"role": "user", "content": summarize_prompt})

            if 'Anthropic' in globals(): # Anthropic uses system parameter
                summary = get_llm_response(messages_task1, llm_model_name, temp=0.3, max_response_tokens=100, system_msg=system_msg_task1)
            else: # OpenAI and Gemini (where system is embedded or direct messages)
                summary = get_llm_response(messages_task1, llm_model_name, temp=0.3, max_response_tokens=100)

            print("\nLLM Summary from File:")
            print(summary)
            ```

          * **Run the script.** Confirm it reads the file and produces a summary.

    4.  **Task 2: Reading from a Web Page & Extracting Information (20 minutes):**

          * **Goal:** Fetch content from a public webpage, extract relevant text, send it to an LLM, and extract structured data.

          * **Add this as a separate section to your script:**

            ```python
            # ... (previous code including get_llm_response) ...

            # --- Task 2: Read from Web & Extract Structured Data ---
            print("\n--- Task 2: Extracting from Web Content ---")
            web_url = "https://www.gutenberg.org/files/1342/1342-0.txt" # Pride and Prejudice (plain text) - easy to parse
            # A more complex webpage might need more advanced BeautifulSoup parsing.
            try:
                response = requests.get(web_url)
                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
                web_text_raw = response.text

                # For Project Gutenberg, we can usually just take the raw text
                # For a normal HTML page, you'd do:
                # soup = BeautifulSoup(web_text_raw, 'html.parser')
                # # Example: extract text from paragraphs, divs, etc.
                # relevant_text = ' '.join([p.get_text() for p in soup.find_all('p')])
                # For this plain text, let's just take a snippet.
                relevant_text = web_text_raw[500:2500] # Take a small snippet to save tokens
                print(f"Successfully fetched {len(web_text_raw)} characters from {web_url}")
                print(f"Using {len(relevant_text)} characters for LLM processing (snippet).")

            except requests.exceptions.RequestException as e:
                print(f"Error fetching web page: {e}")
                exit()

            # Construct prompt for structured extraction from web text
            extract_prompt = f"""
            From the following text snippet from a classic novel, identify the main character's name and any significant locations mentioned in this specific snippet.
            Return the information as a JSON object with 'main_character' (string) and 'locations' (list of strings).
            If no location is found in the snippet, use an empty list.

            Text Snippet:
            ---
            {relevant_text}
            ---
            """
            messages_task2 = []
            system_msg_task2 = "You are an expert at extracting structured information from text."
            if 'OpenAI' in globals(): messages_task2.append({"role": "system", "content": system_msg_task2})
            messages_task2.append({"role": "user", "content": extract_prompt})

            if 'Anthropic' in globals():
                extracted_json_str = get_llm_response(messages_task2, llm_model_name, temp=0.0, max_response_tokens=150, system_msg=system_msg_task2, json_format=True)
            else:
                extracted_json_str = get_llm_response(messages_task2, llm_model_name, temp=0.0, max_response_tokens=150, json_format=True)

            print("\nLLM's JSON Output from Web Content (raw):")
            print(extracted_json_str)

            try:
                import json
                parsed_web_data = json.loads(extracted_json_str)
                print("\nParsed Web Data:")
                print(f"Main Character: {parsed_web_data.get('main_character', 'N/A')}")
                print(f"Locations: {', '.join(parsed_web_data.get('locations', []))}")
            except json.JSONDecodeError as e:
                print(f"Error parsing JSON from web extraction: {e}")
            except Exception as e:
                print(f"An unexpected error occurred during parsing: {e}")
            ```

          * **Run the script.** Verify it fetches text, and the LLM extracts structured data from it.

    5.  **Task 3: Writing LLM Output to a New File (10 minutes):**

          * **Goal:** Take the summary from Task 1 and save it to a new `.txt` file.

          * **Add this to the end of your script (after Task 1's summary is generated):**

            ```python
            # ... (after Task 1's summary generation) ...

            # --- Task 3: Writing LLM Output to File ---
            output_file_path = "mars_article_summary.txt"
            try:
                with open(output_file_path, 'w', encoding='utf-8') as f:
                    f.write(summary) # 'summary' variable from Task 1
                print(f"\nSuccessfully wrote summary to '{output_file_path}'")
            except Exception as e:
                print(f"Error writing summary to file: {e}")
            ```

          * **Run the script.** Check your `applied_llm_course` folder for the new `mars_article_summary.txt` file containing the summary.

  * **Conclusion of Hands-on Session:**

      * You've successfully integrated LLMs with file system and web interactions. You can now read content from local files and web pages, process it with an LLM, and save the generated output back to a file. This is fundamental for building applications that interact with real-world data sources.

-----

### **Homework for Hour 8**

  * **Exercise 1: Web Page Article Summarizer:**
      * Create a new Python file `week2_hour8_homework_web_summarizer.py`.
      * Ask the user (using `input()`) for a URL to a news article (e.g., from BBC News, The Guardian, NYT - ensure it's a public page).
      * Use `requests` to fetch the HTML content.
      * Use `BeautifulSoup` to parse the HTML and extract the main article text (you'll need to inspect the webpage's HTML structure to find common tags like `<article>`, `<div>` with specific classes, or `<p>` tags). *Hint: Look for the main content block, usually a `div` or `article` tag with a specific class, then extract all paragraph text within it.*
      * Send the extracted text to your chosen LLM and ask for a 3-sentence summary of the article.
      * Save the original article URL, the extracted text, and the LLM's summary to a new text file (e.g., `article_summary_[timestamp].txt`).
      * **Submit:** Your `week2_hour8_homework_web_summarizer.py` script and an example output file.
  * **Exercise 2: JSON to HTML List:**
      * Imagine you've extracted a list of products (name, price) as a JSON object using an LLM (similar to Hour 4 homework).
      * Write a script that takes a Python dictionary representing this JSON (you can hardcode it for this exercise, e.g., `products = [{"name": "Laptop", "price": 1200}, {"name": "Mouse", "price": 25}]`).
      * Prompt the LLM to convert this product data into a simple HTML unordered list (`<ul><li>...</li></ul>`).
      * Print the generated HTML.
      * **Submit:** Your script and the HTML output.
  * **Exercise 3: Potential Input/Output Challenges:**
      * Brainstorm two potential issues or challenges you might encounter when:
        1.  Reading text from *any* arbitrary webpage (not just Project Gutenberg's clean text).
        2.  Saving LLM-generated output to a file that might already exist.
      * Briefly explain each challenge and suggest a simple programmatic approach to mitigate it.

-----

You're now connecting LLMs to the wider world of data\! This is a huge step towards building truly integrated and useful applications. Keep up the excellent progress as we move into the final hours of Week 2\!