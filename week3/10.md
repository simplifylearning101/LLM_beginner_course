
### **Week 3: Hour 9 - Optimizing LLM Applications: Cost and Latency**

#### **30% Theory: Beyond Functionality** üí∞

Building a functional LLM application is the first step. Building a production-ready one requires a focus on cost and performance. This hour teaches you how to be a responsible developer by optimizing your applications for real-world usage.

  * **Cost Management:**
      * **Token Economics:** You are charged per token. This includes both the tokens in your prompt and the tokens in the LLM's response. A longer prompt or a longer response costs more.
      * **Strategies:**
          * **Prudent Context:** Keep your conversation history and context as short as possible. Use summarization tools to condense long conversations.
          * **Model Selection:** Use cheaper, smaller models (e.g., GPT-3.5) for simpler tasks like summarization or extraction, and only use more powerful, expensive models (e.g., GPT-4) for complex, high-stakes tasks.
          * **`max_tokens`:** Always set a `max_tokens` limit on your outputs to prevent runaway generation.
          * **Caching:** Store LLM responses to repeated or common queries to avoid paying for them again.
  * **Latency Management:**
      * **The Problem:** LLM API calls are a major source of latency. They can take several seconds to complete, which is a poor user experience.
      * **Strategies:**
          * **Streaming Responses:** Instead of waiting for the full response, stream the output token by token. This gives the user the illusion of a faster response.
          * **Prompt Optimization:** Shorter, clearer prompts often result in faster responses.
          * **Asynchronous Calls:** For applications that require multiple, independent LLM calls, use asynchronous programming to run them in parallel.
          * **Caching:** Similar to cost, caching also improves latency by eliminating the need for an API call.

#### **70% Hands-on Session: Token Counting & Streaming**

  * **Objective:** To implement a token counter to estimate cost and to conceptually understand how response streaming works to improve user experience.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour9_optimization.py`.
          * Copy your LLM setup.
          * Install the `tiktoken` library: `pip install tiktoken`. This library is a popular choice for counting tokens for OpenAI models.

    2.  **Task 1: Implement Token Counting:**

          * **Goal:** Write a function to count tokens in a given prompt and estimate its cost.

          * **Add this code:**

            ```python
            # ... (your existing setup code and get_llm_response_robust function) ...
            import tiktoken

            # --- Task 1: Implement Token Counting ---
            def count_tokens(text: str, model_name: str) -> int:
                """Counts tokens in a string using tiktoken."""
                try:
                    encoding = tiktoken.encoding_for_model(model_name)
                except KeyError:
                    encoding = tiktoken.get_encoding("cl100k_base")
                return len(encoding.encode(text))

            # Example usage
            example_prompt = "Tell me a story about a dragon and a wizard who become friends."
            prompt_tokens = count_tokens(example_prompt, LLM_MODEL)

            print(f"\nExample Prompt: '{example_prompt}'")
            print(f"Estimated prompt tokens: {prompt_tokens}")

            # Conceptual cost calculation (prices can change!)
            # GPT-3.5-turbo pricing: $0.0010 / 1K input tokens
            input_cost = (prompt_tokens / 1000) * 0.0010
            print(f"Estimated input cost: ${input_cost:.5f}")

            # Now, simulate a full LLM call and calculate total cost
            llm_response = get_llm_response_robust([{"role": "user", "content": example_prompt}], LLM_MODEL, max_response_tokens=100)
            response_tokens = count_tokens(llm_response, LLM_MODEL)
            output_cost = (response_tokens / 1000) * 0.0020

            print(f"\nLLM Response: '{llm_response}'")
            print(f"Estimated response tokens: {response_tokens}")
            print(f"Estimated output cost: ${output_cost:.5f}")
            print(f"Total estimated cost: ${input_cost + output_cost:.5f}")
            ```

          * **Run the script.** This will give you a concrete understanding of token cost.

    3.  **Task 2: Conceptual Response Streaming:**

          * **Goal:** Understand how to use the streaming feature of an LLM API to improve perceived latency.

          * **Add this code (this is a conceptual example for OpenAI's streaming):**

            ```python
            # --- Task 2: Conceptual Response Streaming ---
            # NOTE: This code is for demonstration and requires a stream-enabled endpoint

            print("\n--- Conceptual Streaming Demo ---")

            def stream_response_concept(prompt_text):
                # This is what a streaming API call would look like
                # stream = client.chat.completions.create(
                #    model=LLM_MODEL,
                #    messages=[{"role": "user", "content": prompt_text}],
                #    stream=True
                # )
                
                print("Assistant (streaming): ", end="", flush=True)
                # for chunk in stream:
                #    if chunk.choices[0].delta.content is not None:
                #        print(chunk.choices[0].delta.content, end="", flush=True)
                print("The response is now streaming...")
                
            # stream_response_concept("Tell me a long, epic story about the history of the moon.")

            print("\n\nBy streaming, the user doesn't wait for the full response to appear,")
            print("giving the perception of a much faster application.")
            ```

  * **Conclusion:** You now have the knowledge and tools to manage the financial and performance aspects of your LLM applications, which is essential for any real-world deployment.

#### **Homework for Hour 9**

  * **Exercise 1: Optimization Plan.**
      * **Goal:** Create a simple optimization plan for the ultimate RAG agent you built in Hour 8.
      * **Submission:** Write a short document (2-3 paragraphs) that outlines:
        1.  **Cost Reduction:** Two specific strategies you would implement to reduce the agent's cost (e.g., changing models for specific steps, shortening prompts).
        2.  **Latency Improvement:** Two specific strategies you would use to make the agent feel faster to the user (e.g., implementing streaming, optimizing the RAG retrieval).

-----

### **Week 3: Hour 10 - Final Project Planning & Review of Week 3**

#### **30% Theory: The Big Picture & The Road Ahead** üó∫Ô∏è

This final hour is a comprehensive review and a look forward. We'll recap the incredible progress you've made and prepare for the next major phase of your development: building a full-fledged, end-to-end application.

  * **Review of Week 3:**

      * **Advanced Prompting:** You've moved from single-turn prompts to multi-step LLM chains that solve complex problems.
      * **Agents:** You've learned the core agentic loop and empowered an LLM to use external tools.
      * **RAG:** You've built a powerful RAG system to ground LLMs in private data, making them factual and reliable.
      * **Synthesis:** You've combined these concepts into a single, sophisticated agent.
      * **Optimization:** You've learned how to measure and improve your applications' cost and performance.

  * **The Bridge to Week 4:** Next week, we'll take your ultimate agent and build a complete application around it. This means moving from a command-line script to a full-stack application with a backend web server and a user-friendly frontend interface. This transition will solidify your skills and result in a portfolio-worthy project.

#### **70% Hands-on Session: Final Project Planning**

  * **Objective:** To brainstorm and outline a final project that will serve as the culmination of your learning.

  * **Step-by-Step Instructions:**

    1.  **Task 1: Brainstorming a Project Idea.**

          * **Goal:** Think of a simple, useful application that combines at least two of the techniques you learned this week (e.g., RAG + Agents, Chaining + Web I/O).
          * **Prompts to get you thinking:**
              * An agent that reads a recipe URL, extracts ingredients and instructions, and then generates a shopping list.
              * A RAG-powered chatbot for a fictional band's lore and discography.
              * An app that takes a user's resume, fine-tuned to a certain job, and generates a personalized cover letter.

    2.  **Task 2: Fill Out the Project Planning Template.**

          * **Goal:** Complete the following template with a clear, concise plan for your project.

          * **Template:**

            ```
            [Your Name]
            Final Project Plan

            1. Project Title:

            2. Problem Statement: (What problem does this application solve?)

            3. Target Audience: (Who would use this application?)

            4. Core Functionality: (Describe the main feature in 1-2 sentences.)

            5. LLM Techniques to be Used:
               - [Technique 1, e.g., RAG]
               - [Technique 2, e.g., Agents]
               - [Technique 3, e.g., Chaining]

            6. Tooling & Technologies:
               - Backend (e.g., Flask, FastAPI)
               - Frontend (e.g., HTML/CSS, a simple JS framework)
               - LLM Provider: [Your chosen provider]
               - Vector Store (if using RAG): [e.g., ChromaDB, Faiss]

            7. Breakdown of Implementation (High-Level Steps):
               - Week 4, Hour 1: Set up backend API and initial endpoint.
               - Week 4, Hour 2: [Your next step...]
               - Week 4, Hour 3: [Your next step...]
               - etc.
            ```

  * **Conclusion:** This plan will be your guide for Week 4. It marks the end of your formal lesson-based learning for a while and the beginning of your journey as a true LLM application developer.

#### **Homework for Hour 10**

  * **Exercise 1: Final Project Plan Submission.**
      * **Goal:** Submit your completed project planning template. This is the only homework for this hour.
      * **Note:** Your plan will not be graded, but it will be used to help guide you in the next week. Think of it as your personal roadmap to building something great.

I'm now ready to generate the next hours.

### **Week 3: Hour 7 - Fine-Tuning vs. RAG (A Deeper Dive)**

#### **30% Theory: Choosing the Right Tool** üõ†Ô∏è

In the world of LLM applications, two of the most powerful techniques for customizing a model's behavior are RAG and fine-tuning. While we've mastered RAG, it's crucial to understand when to use it and when a different approach might be better. This hour clarifies the key differences, advantages, and use cases for each.

  * **RAG (Retrieval-Augmented Generation):**

      * **Core Idea:** You provide the LLM with relevant, external information at the time of the query. The model's core knowledge and behavior remain unchanged.
      * **Best for:**
          * **Factual Knowledge:** Answering questions based on specific, up-to-date, or private documents (e.g., a company's financial report, a legal brief, a product manual).
          * **Dynamic Data:** When information changes frequently and needs to be updated quickly.
          * **Reducing Hallucinations:** Grounding the LLM's response in a verifiable source.
      * **Pros:** Low cost, fast to implement, easy to update, and avoids "model drift" (where a fine-tuned model loses its general knowledge).
      * **Cons:** Can't change the model's fundamental tone or behavior. The quality of the response is highly dependent on the quality of the retrieved context.

  * **Fine-Tuning:**

      * **Core Idea:** You train the LLM on a custom dataset to update its internal weights and biases. The model learns a new style, persona, or pattern.
      * **Best for:**
          * **Stylistic Changes:** Making the model adopt a specific tone (e.g., a professional, sarcastic, or casual persona).
          * **Specific Task Formats:** Teaching the model to consistently output data in a very specific format (e.g., consistently returning a JSON object with specific keys and values, or consistently summarizing text in a 10-word sentence).
          * **Proprietary Knowledge:** When the knowledge is fundamental to the model's behavior and not just a fact (e.g., training a model on medical jargon so it "thinks" like a doctor).
      * **Pros:** The model's output becomes highly consistent and predictable. It can improve performance on specific tasks.
      * **Cons:** High cost, time-consuming (requires data collection and a training job), requires a large amount of high-quality training data, and is not suitable for dynamic, factual information.

  * **The Hybrid Approach:** In many advanced applications, both techniques are used together. For example, a model might be fine-tuned to adopt a company's specific brand voice, while RAG is used to pull in up-to-date product information from a private database.

-----

#### **70% Hands-on Session: Exploring Fine-Tuning (Conceptual)**

  * **Objective:** To understand the data format and API call for fine-tuning without actually running the expensive and time-consuming process. This will solidify your conceptual understanding and prepare you for a real fine-tuning task in the future.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour7_fine_tuning.py`.
          * We will only be writing sample data and conceptual code here. No LLM client setup is needed for this task.

    2.  **Task 1: Understand the Fine-Tuning Data Format:**

          * **Goal:** Learn how to format a dataset for fine-tuning. A common format is JSONL (JSON Lines), where each line is a JSON object representing a single training example.

          * **Add the following code to your script. This is just a Python representation of the JSONL data:**

            ```python
            import json

            # --- Task 1: Understanding Fine-Tuning Data Format ---
            # This is a conceptual dataset for a model we want to fine-tune to always
            # respond in a sarcastic persona about tech products.

            # Each item in the list is a training example.
            # The 'messages' structure is identical to our chat history.
            training_data = [
                {
                    "messages": [
                        {"role": "system", "content": "You are a highly sarcastic, dry-witted technology reviewer."},
                        {"role": "user", "content": "What's the best new smartphone?"},
                        {"role": "assistant", "content": "Oh, another new smartphone? How original. It's the same rectangle with slightly shinier glass and a camera that takes pictures of your lunch, but with more pixels. Absolutely groundbreaking stuff. My life is complete now."}
                    ]
                },
                {
                    "messages": [
                        {"role": "system", "content": "You are a highly sarcastic, dry-witted technology reviewer."},
                        {"role": "user", "content": "Tell me about the new smart speaker."},
                        {"role": "assistant", "content": "Ah yes, the new smart speaker. Now you can have a soulless monolith listening to your every word, and it only took three generations for them to make it a sphere. It's basically a surveillance device that plays music, a bargain at twice the price."}
                    ]
                },
                # Add more examples here for a full dataset
            ]

            # This is how you would convert it to the required JSONL format:
            with open("sarcastic_reviewer_data.jsonl", "w") as f:
                for entry in training_data:
                    f.write(json.dumps(entry) + '\n')

            print("Successfully created a conceptual fine-tuning data file (sarcastic_reviewer_data.jsonl).")
            ```

    3.  **Task 2: Conceptual Fine-Tuning API Call:**

          * **Goal:** Understand how you would initiate a fine-tuning job on a platform like OpenAI.

          * **Add this conceptual code to the end of your script:**

            ```python
            # --- Task 2: Conceptual Fine-Tuning API Call ---
            # NOTE: This code is for demonstration only and will not work without
            # a real API client and valid file upload.

            # import openai

            # def conceptual_fine_tune_job():
            #     # Step 1: Upload your fine-tuning data file
            #     print("Uploading data file...")
            #     # file_response = openai.files.create(
            #     #    file=open("sarcastic_reviewer_data.jsonl", "rb"),
            #     #    purpose="fine-tune"
            #     # )
            #     # file_id = file_response.id

            #     # Step 2: Create the fine-tuning job
            #     print("Creating fine-tuning job...")
            #     # fine_tune_response = openai.fine_tuning.jobs.create(
            #     #    training_file=file_id,
            #     #    model="gpt-3.5-turbo" # Or another fine-tunable model
            #     # )

            #     # print(f"Fine-tuning job created with ID: {fine_tune_response.id}")
            #     # print("You can monitor its status via the API or your provider's dashboard.")

            # # conceptual_fine_tune_job()

            print("\nConceptual walkthrough of fine-tuning complete. We now understand the data and process.")
            ```

  * **Conclusion:** You now have a solid theoretical and practical understanding of when and how to approach fine-tuning. This knowledge, combined with your RAG skills, gives you a comprehensive toolkit for building truly intelligent and customized LLM applications.

#### **Homework for Hour 7**

  * **Exercise 1: RAG or Fine-Tuning?**
      * **Goal:** Determine the best LLM approach for three different scenarios and justify your choice.
      * **Submission:** Write a brief response for each scenario:
        1.  **Scenario A:** An internal company chatbot needs to answer questions about the latest company-wide memos and policies, which are updated weekly.
        2.  **Scenario B:** A chatbot needs to be able to generate short stories in the unique, poetic style of author Neil Gaiman.
        3.  **Scenario C:** An agent needs to be able to read a user's resume, extract key skills, and then generate a tailored cover letter that matches the tone of a specific job posting.

-----

### **Week 3: Hour 8 - Project: Building a Multi-Step, RAG-Powered Agent**

#### **30% Theory: The Ultimate Synthesis** üß©

This is the capstone project of Week 3. We will combine all the skills you've acquired:

1.  **Agentic Logic** (Hour 2 & 4): The agent will decide which tools to use.
2.  **RAG** (Hour 3): One of the agent's tools will be a search of our private knowledge base.
3.  **Chaining** (Hour 1): Another tool will perform a multi-step task like summarizing a retrieved document.
4.  **Robustness** (Hour 2 & 4): The entire system will be wrapped in a robust loop with error handling.

The agent's main goal will be to answer complex user questions by intelligently using a RAG tool to get facts and a summarization tool to present them concisely.

#### **70% Hands-on Session: The Ultimate Agent**

  * **Objective:** To build a single Python script that implements a multi-step, RAG-powered agent capable of answering complex queries about a private knowledge base.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour8_ultimate_agent.py`.
          * Copy the full `get_llm_response_robust` function and the LLM setup from a previous hour.
          * Copy the RAG setup from Hour 5 (the `knowledge_base`, `embedding_model`, and `retrieve_relevant_context` function).

    2.  **Task 1: Define Your Ultimate Tools:**

          * **Goal:** Create a dictionary that maps tool names to their functions and a detailed description. We'll have two tools: `knowledge_base_search` and `summarize_text`.

          * **Add the following code to your script:**

            ```python
            # ... (your full setup, including RAG functions from Hour 5) ...

            # --- Task 1: Define Ultimate Tools ---
            def knowledge_base_search(query: str, num_docs: int = 1) -> str:
                """
                Searches the internal company knowledge base for relevant documents.
                This is a wrapper around our RAG system.
                """
                retrieved_docs = retrieve_relevant_context(query, k=num_docs)
                return json.dumps({"documents": retrieved_docs})

            def summarize_text(text: str) -> str:
                """
                Summarizes a long piece of text into a concise paragraph.
                This is an LLM chain disguised as a tool.
                """
                # This is a chained LLM call
                summary_prompt = f"Summarize the following text concisely:\n---\n{text}\n---"
                messages_summary = [{"role": "user", "content": summary_prompt}]
                if 'OpenAI' in globals():
                     messages_summary.insert(0, {"role": "system", "content": "You are a professional summarizer."})
                
                return get_llm_response_robust(messages_summary, LLM_MODEL, temp=0.5, max_response_tokens=150)

            # Dictionary to map tool names to functions
            TOOLS = {
                "knowledge_base_search": knowledge_base_search,
                "summarize_text": summarize_text,
            }
            ```

    3.  **Task 2: Craft the Ultimate Agentic Prompt:**

          * **Goal:** Write a prompt that instructs the LLM to intelligently use the RAG tool first, and then use the summarization tool if needed.

          * **Add this code:**

            ```python
            # ... (code from Task 1) ...

            # The ultimate agentic prompt
            AGENT_PROMPT = """
            You are a helpful and knowledgeable assistant for our company. You have access to two tools.
            Always use the 'knowledge_base_search' tool first to find relevant information before answering.

            Name: knowledge_base_search
            Description: Searches the company's internal knowledge base for information. Returns a list of relevant documents.
            Parameters:
            - query (string, required): The search query.
            - num_docs (integer, optional, default=1): The number of documents to retrieve.

            Name: summarize_text
            Description: Summarizes a long text into a concise paragraph. Use this if the retrieved document is too long.
            Parameters:
            - text (string, required): The text to summarize.

            To use a tool, you MUST follow this exact, single-line format:
            <tool_call> TOOL_NAME="[tool_name]" PARAMETERS="{[param_name]":"[param_value]", ...}" </tool_call>

            If you can answer the question directly, or once you have used a tool and have the result, provide a final answer in natural language based on the context you have gathered.
            If the answer is not available in the knowledge base, politely say so.
            """
            ```

    4.  **Task 3: Implement the Ultimate Agentic Loop:**

          * **Goal:** Combine the advanced parsing loop from Hour 4 with the new toolset.

          * **Add this code to the end of your script (this is similar to Hour 4, but with the new tools):**

            ```python
            # ... (code from Task 1 and 2) ...
            import re

            print("\n--- The Ultimate RAG-Powered Agent ---")
            print("I can answer questions about company policies and facts.")
            print("Type 'quit' or 'exit' to end.")

            while True:
                user_input = input("\nYou: ").strip()
                if user_input.lower() in ["quit", "exit"]:
                    print("Goodbye!")
                    break

                messages = [{"role": "user", "content": AGENT_PROMPT + f"\nUser Question: {user_input}"}]
                if 'OpenAI' in globals():
                    messages.insert(0, {"role": "system", "content": "You are a helpful assistant with access to tools."})
                
                try:
                    raw_response = get_llm_response_robust(messages, LLM_MODEL, temp=0.0, max_response_tokens=250)
                    print(f"\nDEBUG: Raw LLM Output -> {raw_response}")
                    
                    tool_call_match = re.search(r'<tool_call>.*</tool_call>', raw_response)
                    
                    if tool_call_match:
                        tool_call_str = tool_call_match.group(0)
                        tool_name_match = re.search(r'TOOL_NAME="([^"]+)"', tool_call_str)
                        params_match = re.search(r'PARAMETERS="([^"]+)"', tool_call_str)
                        
                        if tool_name_match and params_match:
                            tool_name = tool_name_match.group(1)
                            params_str = params_match.group(1).replace("'", '"')
                            
                            try:
                                params = json.loads(params_str)
                                print(f"DEBUG: Calling tool '{tool_name}' with parameters {params}")
                                
                                if tool_name in TOOLS:
                                    tool_result = TOOLS[tool_name](**params)
                                else:
                                    tool_result = f"Error: Tool '{tool_name}' not found."
                                
                                # Feed the result back to the LLM
                                final_answer_prompt = f"""
                                Based on this tool's result, answer the user's original question: '{user_input}'
                                Tool Result: {tool_result}
                                """
                                messages_final = [{"role": "user", "content": AGENT_PROMPT + final_answer_prompt}]
                                if 'OpenAI' in globals():
                                    messages_final.insert(0, {"role": "system", "content": "You are a helpful assistant with access to tools."})
                                
                                final_answer = get_llm_response_robust(messages_final, LLM_MODEL, temp=0.0, max_response_tokens=250)
                                print("Assistant:", final_answer)
                            except (json.JSONDecodeError, Exception) as e:
                                print(f"Assistant: An error occurred during tool execution: {e}")
                        else:
                            print("Assistant: I'm sorry, I couldn't understand the tool call.")
                    else:
                        print("Assistant:", raw_response)
                except Exception as e:
                    print(f"Assistant: An unexpected error occurred: {e}")
            ```

          * **Run the script.** Test with queries like:

              * "What is our return policy?"
              * "Who is the tech support contact?"
              * "Summarize the company's contact information."
              * "Tell me about the software update."

  * **Conclusion:** You have built a truly sophisticated LLM application. By combining the power of agents, RAG, and chaining, you've created a system that can reason about a task, use a knowledge base, and present information concisely.

#### **Homework for Hour 8**

  * **Exercise 1: Add a "User Preferences" Tool.**
      * **Goal:** Add a new, simple tool to your `ultimate_agent.py` that simulates a user's long-term memory.
      * **Task:** Create a new Python function `get_user_preferences(user_id: str)` that returns a hardcoded string of preferences (e.g., "The user prefers answers that are short and to the point."). Update your `AGENT_PROMPT` to include this new tool.
      * Modify the agent's logic to retrieve these preferences and use them in the final LLM call to tailor the response.
      * **Submit:** Your modified script and a log of a conversation showing the agent correctly using the new tool to influence its final answer.

-----

### **Week 3: Hour 9 - Optimizing LLM Applications: Cost and Latency**

#### **30% Theory: Beyond Functionality** üí∞

Building a functional LLM application is the first step. Building a production-ready one requires a focus on cost and performance. This hour teaches you how to be a responsible developer by optimizing your applications for real-world usage.

  * **Cost Management:**
      * **Token Economics:** You are charged per token. This includes both the tokens in your prompt and the tokens in the LLM's response. A longer prompt or a longer response costs more.
      * **Strategies:**
          * **Prudent Context:** Keep your conversation history and context as short as possible. Use summarization tools to condense long conversations.
          * **Model Selection:** Use cheaper, smaller models (e.g., GPT-3.5) for simpler tasks like summarization or extraction, and only use more powerful, expensive models (e.g., GPT-4) for complex, high-stakes tasks.
          * **`max_tokens`:** Always set a `max_tokens` limit on your outputs to prevent runaway generation.
          * **Caching:** Store LLM responses to repeated or common queries to avoid paying for them again.
  * **Latency Management:**
      * **The Problem:** LLM API calls are a major source of latency. They can take several seconds to complete, which is a poor user experience.
      * **Strategies:**
          * **Streaming Responses:** Instead of waiting for the full response, stream the output token by token. This gives the user the illusion of a faster response.
          * **Prompt Optimization:** Shorter, clearer prompts often result in faster responses.
          * **Asynchronous Calls:** For applications that require multiple, independent LLM calls, use asynchronous programming to run them in parallel.
          * **Caching:** Similar to cost, caching also improves latency by eliminating the need for an API call.

#### **70% Hands-on Session: Token Counting & Streaming**

  * **Objective:** To implement a token counter to estimate cost and to conceptually understand how response streaming works to improve user experience.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour9_optimization.py`.
          * Copy your LLM setup.
          * Install the `tiktoken` library: `pip install tiktoken`. This library is a popular choice for counting tokens for OpenAI models.

    2.  **Task 1: Implement Token Counting:**

          * **Goal:** Write a function to count tokens in a given prompt and estimate its cost.

          * **Add this code:**

            ```python
            # ... (your existing setup code and get_llm_response_robust function) ...
            import tiktoken

            # --- Task 1: Implement Token Counting ---
            def count_tokens(text: str, model_name: str) -> int:
                """Counts tokens in a string using tiktoken."""
                try:
                    encoding = tiktoken.encoding_for_model(model_name)
                except KeyError:
                    encoding = tiktoken.get_encoding("cl100k_base")
                return len(encoding.encode(text))

            # Example usage
            example_prompt = "Tell me a story about a dragon and a wizard who become friends."
            prompt_tokens = count_tokens(example_prompt, LLM_MODEL)

            print(f"\nExample Prompt: '{example_prompt}'")
            print(f"Estimated prompt tokens: {prompt_tokens}")

            # Conceptual cost calculation (prices can change!)
            # GPT-3.5-turbo pricing: $0.0010 / 1K input tokens
            input_cost = (prompt_tokens / 1000) * 0.0010
            print(f"Estimated input cost: ${input_cost:.5f}")

            # Now, simulate a full LLM call and calculate total cost
            llm_response = get_llm_response_robust([{"role": "user", "content": example_prompt}], LLM_MODEL, max_response_tokens=100)
            response_tokens = count_tokens(llm_response, LLM_MODEL)
            output_cost = (response_tokens / 1000) * 0.0020

            print(f"\nLLM Response: '{llm_response}'")
            print(f"Estimated response tokens: {response_tokens}")
            print(f"Estimated output cost: ${output_cost:.5f}")
            print(f"Total estimated cost: ${input_cost + output_cost:.5f}")
            ```

          * **Run the script.** This will give you a concrete understanding of token cost.

    3.  **Task 2: Conceptual Response Streaming:**

          * **Goal:** Understand how to use the streaming feature of an LLM API to improve perceived latency.

          * **Add this code (this is a conceptual example for OpenAI's streaming):**

            ```python
            # --- Task 2: Conceptual Response Streaming ---
            # NOTE: This code is for demonstration and requires a stream-enabled endpoint

            print("\n--- Conceptual Streaming Demo ---")

            def stream_response_concept(prompt_text):
                # This is what a streaming API call would look like
                # stream = client.chat.completions.create(
                #    model=LLM_MODEL,
                #    messages=[{"role": "user", "content": prompt_text}],
                #    stream=True
                # )
                
                print("Assistant (streaming): ", end="", flush=True)
                # for chunk in stream:
                #    if chunk.choices[0].delta.content is not None:
                #        print(chunk.choices[0].delta.content, end="", flush=True)
                print("The response is now streaming...")
                
            # stream_response_concept("Tell me a long, epic story about the history of the moon.")

            print("\n\nBy streaming, the user doesn't wait for the full response to appear,")
            print("giving the perception of a much faster application.")
            ```

  * **Conclusion:** You now have the knowledge and tools to manage the financial and performance aspects of your LLM applications, which is essential for any real-world deployment.

#### **Homework for Hour 9**

  * **Exercise 1: Optimization Plan.**
      * **Goal:** Create a simple optimization plan for the ultimate RAG agent you built in Hour 8.
      * **Submission:** Write a short document (2-3 paragraphs) that outlines:
        1.  **Cost Reduction:** Two specific strategies you would implement to reduce the agent's cost (e.g., changing models for specific steps, shortening prompts).
        2.  **Latency Improvement:** Two specific strategies you would use to make the agent feel faster to the user (e.g., implementing streaming, optimizing the RAG retrieval).

-----

### **Week 3: Hour 10 - Final Project Planning & Review of Week 3**

#### **30% Theory: The Big Picture & The Road Ahead** üó∫Ô∏è

This final hour is a comprehensive review and a look forward. We'll recap the incredible progress you've made and prepare for the next major phase of your development: building a full-fledged, end-to-end application.

  * **Review of Week 3:**

      * **Advanced Prompting:** You've moved from single-turn prompts to multi-step LLM chains that solve complex problems.
      * **Agents:** You've learned the core agentic loop and empowered an LLM to use external tools.
      * **RAG:** You've built a powerful RAG system to ground LLMs in private data, making them factual and reliable.
      * **Synthesis:** You've combined these concepts into a single, sophisticated agent.
      * **Optimization:** You've learned how to measure and improve your applications' cost and performance.

  * **The Bridge to Week 4:** Next week, we'll take your ultimate agent and build a complete application around it. This means moving from a command-line script to a full-stack application with a backend web server and a user-friendly frontend interface. This transition will solidify your skills and result in a portfolio-worthy project.

#### **70% Hands-on Session: Final Project Planning**

  * **Objective:** To brainstorm and outline a final project that will serve as the culmination of your learning.

  * **Step-by-Step Instructions:**

    1.  **Task 1: Brainstorming a Project Idea.**

          * **Goal:** Think of a simple, useful application that combines at least two of the techniques you learned this week (e.g., RAG + Agents, Chaining + Web I/O).
          * **Prompts to get you thinking:**
              * An agent that reads a recipe URL, extracts ingredients and instructions, and then generates a shopping list.
              * A RAG-powered chatbot for a fictional band's lore and discography.
              * An app that takes a user's resume, fine-tuned to a certain job, and generates a personalized cover letter.

    2.  **Task 2: Fill Out the Project Planning Template.**

          * **Goal:** Complete the following template with a clear, concise plan for your project.

          * **Template:**

            ```
            [Your Name]
            Final Project Plan

            1. Project Title:

            2. Problem Statement: (What problem does this application solve?)

            3. Target Audience: (Who would use this application?)

            4. Core Functionality: (Describe the main feature in 1-2 sentences.)

            5. LLM Techniques to be Used:
               - [Technique 1, e.g., RAG]
               - [Technique 2, e.g., Agents]
               - [Technique 3, e.g., Chaining]

            6. Tooling & Technologies:
               - Backend (e.g., Flask, FastAPI)
               - Frontend (e.g., HTML/CSS, a simple JS framework)
               - LLM Provider: [Your chosen provider]
               - Vector Store (if using RAG): [e.g., ChromaDB, Faiss]

            7. Breakdown of Implementation (High-Level Steps):
               - Week 4, Hour 1: Set up backend API and initial endpoint.
               - Week 4, Hour 2: [Your next step...]
               - Week 4, Hour 3: [Your next step...]
               - etc.
            ```

  * **Conclusion:** This plan will be your guide for Week 4. It marks the end of your formal lesson-based learning for a while and the beginning of your journey as a true LLM application developer.

#### **Homework for Hour 10**

  * **Exercise 1: Final Project Plan Submission.**
      * **Goal:** Submit your completed project planning template. This is the only homework for this hour.
      * **Note:** Your plan will not be graded, but it will be used to help guide you in the next week. Think of it as your personal roadmap to building something great.