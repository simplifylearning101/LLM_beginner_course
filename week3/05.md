
### **Week 5: Project - Building a RAG-Powered Chatbot**

#### **30% Theory: The RAG Chatbot: A Synthesis of Skills** ðŸ§ 

This hour is a project-based synthesis of the concepts you've learned so far. We will combine the conversational agent framework from Week 2 with the RAG pipeline from Week 3 to create a chatbot that can answer questions based on a private, up-to-date knowledge base.

  * **The Problem:** Our chatbot from Week 2 had no memory of external information. Its knowledge was limited to its training data. If you asked it about a new event or a private document, it would hallucinate or say it didn't know.
  * **The Solution: RAG Integration:** By integrating RAG into the chatbot loop, we solve this problem.
    1.  The user asks a question.
    2.  Your code **retrieves** relevant documents from your vector store.
    3.  The retrieved documents are added to the LLM's prompt as **context**.
    4.  The LLM **generates** a response, now "grounded" in the factual information you provided.
  * **The Pipeline in a Loop:**
    1.  User input is received.
    2.  **RAG Step:** The input is used to query a vector store, which returns a relevant text chunk.
    3.  **Prompt Engineering Step:** The retrieved text is inserted into the main chat prompt.
    4.  **LLM Call:** The full prompt (conversation history + retrieved text) is sent to the LLM.
    5.  **Output:** The LLM's response is presented to the user.
        This continuous loop allows the chatbot to have a persistent memory of the conversation *and* access to a dynamic external knowledge base.

-----

#### **70% Hands-on Session: Building a Full RAG Chatbot**

  * **Objective:** To combine your existing chatbot and RAG scripts into a single, functional application that answers questions based on a knowledge base.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour5_rag_chatbot.py`.
          * Copy the robust LLM setup from Week 2, Hour 9 (`get_llm_response_robust`).
          * Copy the RAG-related imports from Week 3, Hour 3 (`SentenceTransformer`, `numpy`).
          * We will re-use the `retrieve_relevant_context` function and the knowledge base from Hour 3.

    2.  **Task 1: The RAG-Chatbot Setup:**

          * **Goal:** Define the knowledge base and initialize the embedding model and vector store at the start of the script.

          * **Add the following code:**

            ```python
            # ... (your existing setup code) ...
            from sentence_transformers import SentenceTransformer
            import numpy as np

            # --- Task 1: RAG Setup ---
            # Your knowledge base (a simple company FAQ)
            knowledge_base = [
                "The primary contact for technical support is Sarah Chen, who can be reached at sarah.chen@company.com.",
                "Our return policy states that products can be returned within 30 days of purchase for a full refund.",
                "The new software update (Version 2.1) includes enhanced security features and a redesigned user interface.",
                "Company headquarters are located at 123 Tech Blvd, Silicon Valley, CA.",
                "Customer service hours are Monday through Friday, 9 AM to 5 PM local time."
            ]

            # Load the embedding model and create the vector store
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            embeddings = embedding_model.encode(knowledge_base)
            vector_store = list(zip(knowledge_base, embeddings))

            def retrieve_relevant_context(query, k=1):
                """Finds the top k most relevant documents to a query."""
                query_embedding = embedding_model.encode(query)
                similarities = np.dot(embeddings, query_embedding)
                top_k_indices = np.argsort(similarities)[-k:]
                retrieved_docs = [knowledge_base[i] for i in top_k_indices]
                return retrieved_docs
            ```

    3.  **Task 2: The Chatbot Loop with RAG Integration:**

          * **Goal:** Modify a standard chatbot loop to perform the RAG retrieval before each LLM call.

          * **Add this code:**

            ```python
            # ... (code from Task 1) ...

            # --- Task 2: The Chatbot Loop with RAG ---
            print("\n--- RAG-Powered Company Chatbot ---")
            print("I can answer questions about company policies and facts.")
            print("Type 'quit' or 'exit' to end the conversation.")

            conversation_history = []

            # Define chatbot's persona
            system_instruction = "You are a helpful assistant for CompanyX. You answer user questions concisely and professionally, referring to the provided context when possible. If a question is not related to the context, politely state that you can only answer questions about the provided documents."

            if 'OpenAI' in globals():
                conversation_history.append({"role": "system", "content": system_instruction})

            while True:
                user_input = input("\nYou: ").strip()
                if user_input.lower() in ["quit", "exit"]:
                    print("Goodbye!")
                    break

                # Step 1: Perform RAG retrieval based on user input
                retrieved_context = retrieve_relevant_context(user_input, k=1)
                
                # Step 2: Build a prompt that includes the context
                context_prompt = f"""
                You have been provided with the following context:
                ---
                {retrieved_context[0]}
                ---
                Use this to answer the user's question: {user_input}
                """
                
                # Step 3: Add the context-augmented prompt to the conversation history
                # We do this instead of adding just the user input to ground the LLM
                conversation_history.append({"role": "user", "content": context_prompt})
                
                # We can also add a safety check here to prevent the conversation history from getting too long.
                # For this simple example, we'll keep it as is.
                
                try:
                    # Step 4: Get the LLM's response
                    if 'Anthropic' in globals():
                         llm_response = get_llm_response_robust(conversation_history, LLM_MODEL, temp=0.2, max_response_tokens=150, system_msg=system_instruction)
                    else:
                         llm_response = get_llm_response_robust(conversation_history, LLM_MODEL, temp=0.2, max_response_tokens=150)

                    print(f"Assistant: {llm_response}")

                    # Step 5: Add the assistant's response to the history
                    conversation_history.append({"role": "assistant", "content": llm_response})
                    
                except Exception as e:
                    print(f"Assistant: An error occurred: {e}")
                    conversation_history.pop() # Remove the user message on failure
            ```

          * **Run the script.** Test it with questions like:

              * "What's the support email?" (should get Sarah Chen's email)
              * "What's the company's address?"
              * "Tell me about the new software update."
              * "What's a good recipe for lasagna?" (should be rejected as out of scope)

  * **Conclusion:** You have successfully built a powerful RAG-powered chatbot. This is a foundational, real-world application that can be applied to countless business use cases, from customer service to internal knowledge management.

#### **Homework for Hour 5**

  * **Exercise 1: Expand and Refine Your RAG Chatbot.**
      * Find a small public document online (e.g., a short Wikipedia article or a company's "About Us" page) and replace the existing `knowledge_base` with its content.
      * Modify the `system_instruction` to reflect the new topic.
      * Add a simple context management rule to your `while` loop to ensure the `conversation_history` does not grow indefinitely. (You can use the token management approach from Week 2, Hour 7).
      * **Submit:** Your modified script and a log of a short conversation demonstrating that the chatbot can answer questions based on the new knowledge base.

-----
