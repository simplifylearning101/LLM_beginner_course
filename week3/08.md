
### **Week 3: Hour 8 - Project: Building a Multi-Step, RAG-Powered Agent**

#### **30% Theory: The Ultimate Synthesis** ðŸ§©

This is the capstone project of Week 3. We will combine all the skills you've acquired:

1.  **Agentic Logic** (Hour 2 & 4): The agent will decide which tools to use.
2.  **RAG** (Hour 3): One of the agent's tools will be a search of our private knowledge base.
3.  **Chaining** (Hour 1): Another tool will perform a multi-step task like summarizing a retrieved document.
4.  **Robustness** (Hour 2 & 4): The entire system will be wrapped in a robust loop with error handling.

The agent's main goal will be to answer complex user questions by intelligently using a RAG tool to get facts and a summarization tool to present them concisely.

#### **70% Hands-on Session: The Ultimate Agent**

  * **Objective:** To build a single Python script that implements a multi-step, RAG-powered agent capable of answering complex queries about a private knowledge base.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour8_ultimate_agent.py`.
          * Copy the full `get_llm_response_robust` function and the LLM setup from a previous hour.
          * Copy the RAG setup from Hour 5 (the `knowledge_base`, `embedding_model`, and `retrieve_relevant_context` function).

    2.  **Task 1: Define Your Ultimate Tools:**

          * **Goal:** Create a dictionary that maps tool names to their functions and a detailed description. We'll have two tools: `knowledge_base_search` and `summarize_text`.

          * **Add the following code to your script:**

            ```python
            # ... (your full setup, including RAG functions from Hour 5) ...

            # --- Task 1: Define Ultimate Tools ---
            def knowledge_base_search(query: str, num_docs: int = 1) -> str:
                """
                Searches the internal company knowledge base for relevant documents.
                This is a wrapper around our RAG system.
                """
                retrieved_docs = retrieve_relevant_context(query, k=num_docs)
                return json.dumps({"documents": retrieved_docs})

            def summarize_text(text: str) -> str:
                """
                Summarizes a long piece of text into a concise paragraph.
                This is an LLM chain disguised as a tool.
                """
                # This is a chained LLM call
                summary_prompt = f"Summarize the following text concisely:\n---\n{text}\n---"
                messages_summary = [{"role": "user", "content": summary_prompt}]
                if 'OpenAI' in globals():
                     messages_summary.insert(0, {"role": "system", "content": "You are a professional summarizer."})
                
                return get_llm_response_robust(messages_summary, LLM_MODEL, temp=0.5, max_response_tokens=150)

            # Dictionary to map tool names to functions
            TOOLS = {
                "knowledge_base_search": knowledge_base_search,
                "summarize_text": summarize_text,
            }
            ```

    3.  **Task 2: Craft the Ultimate Agentic Prompt:**

          * **Goal:** Write a prompt that instructs the LLM to intelligently use the RAG tool first, and then use the summarization tool if needed.

          * **Add this code:**

            ```python
            # ... (code from Task 1) ...

            # The ultimate agentic prompt
            AGENT_PROMPT = """
            You are a helpful and knowledgeable assistant for our company. You have access to two tools.
            Always use the 'knowledge_base_search' tool first to find relevant information before answering.

            Name: knowledge_base_search
            Description: Searches the company's internal knowledge base for information. Returns a list of relevant documents.
            Parameters:
            - query (string, required): The search query.
            - num_docs (integer, optional, default=1): The number of documents to retrieve.

            Name: summarize_text
            Description: Summarizes a long text into a concise paragraph. Use this if the retrieved document is too long.
            Parameters:
            - text (string, required): The text to summarize.

            To use a tool, you MUST follow this exact, single-line format:
            <tool_call> TOOL_NAME="[tool_name]" PARAMETERS="{[param_name]":"[param_value]", ...}" </tool_call>

            If you can answer the question directly, or once you have used a tool and have the result, provide a final answer in natural language based on the context you have gathered.
            If the answer is not available in the knowledge base, politely say so.
            """
            ```

    4.  **Task 3: Implement the Ultimate Agentic Loop:**

          * **Goal:** Combine the advanced parsing loop from Hour 4 with the new toolset.

          * **Add this code to the end of your script (this is similar to Hour 4, but with the new tools):**

            ```python
            # ... (code from Task 1 and 2) ...
            import re

            print("\n--- The Ultimate RAG-Powered Agent ---")
            print("I can answer questions about company policies and facts.")
            print("Type 'quit' or 'exit' to end.")

            while True:
                user_input = input("\nYou: ").strip()
                if user_input.lower() in ["quit", "exit"]:
                    print("Goodbye!")
                    break

                messages = [{"role": "user", "content": AGENT_PROMPT + f"\nUser Question: {user_input}"}]
                if 'OpenAI' in globals():
                    messages.insert(0, {"role": "system", "content": "You are a helpful assistant with access to tools."})
                
                try:
                    raw_response = get_llm_response_robust(messages, LLM_MODEL, temp=0.0, max_response_tokens=250)
                    print(f"\nDEBUG: Raw LLM Output -> {raw_response}")
                    
                    tool_call_match = re.search(r'<tool_call>.*</tool_call>', raw_response)
                    
                    if tool_call_match:
                        tool_call_str = tool_call_match.group(0)
                        tool_name_match = re.search(r'TOOL_NAME="([^"]+)"', tool_call_str)
                        params_match = re.search(r'PARAMETERS="([^"]+)"', tool_call_str)
                        
                        if tool_name_match and params_match:
                            tool_name = tool_name_match.group(1)
                            params_str = params_match.group(1).replace("'", '"')
                            
                            try:
                                params = json.loads(params_str)
                                print(f"DEBUG: Calling tool '{tool_name}' with parameters {params}")
                                
                                if tool_name in TOOLS:
                                    tool_result = TOOLS[tool_name](**params)
                                else:
                                    tool_result = f"Error: Tool '{tool_name}' not found."
                                
                                # Feed the result back to the LLM
                                final_answer_prompt = f"""
                                Based on this tool's result, answer the user's original question: '{user_input}'
                                Tool Result: {tool_result}
                                """
                                messages_final = [{"role": "user", "content": AGENT_PROMPT + final_answer_prompt}]
                                if 'OpenAI' in globals():
                                    messages_final.insert(0, {"role": "system", "content": "You are a helpful assistant with access to tools."})
                                
                                final_answer = get_llm_response_robust(messages_final, LLM_MODEL, temp=0.0, max_response_tokens=250)
                                print("Assistant:", final_answer)
                            except (json.JSONDecodeError, Exception) as e:
                                print(f"Assistant: An error occurred during tool execution: {e}")
                        else:
                            print("Assistant: I'm sorry, I couldn't understand the tool call.")
                    else:
                        print("Assistant:", raw_response)
                except Exception as e:
                    print(f"Assistant: An unexpected error occurred: {e}")
            ```

          * **Run the script.** Test with queries like:

              * "What is our return policy?"
              * "Who is the tech support contact?"
              * "Summarize the company's contact information."
              * "Tell me about the software update."

  * **Conclusion:** You have built a truly sophisticated LLM application. By combining the power of agents, RAG, and chaining, you've created a system that can reason about a task, use a knowledge base, and present information concisely.

#### **Homework for Hour 8**

  * **Exercise 1: Add a "User Preferences" Tool.**
      * **Goal:** Add a new, simple tool to your `ultimate_agent.py` that simulates a user's long-term memory.
      * **Task:** Create a new Python function `get_user_preferences(user_id: str)` that returns a hardcoded string of preferences (e.g., "The user prefers answers that are short and to the point."). Update your `AGENT_PROMPT` to include this new tool.
      * Modify the agent's logic to retrieve these preferences and use them in the final LLM call to tailor the response.
      * **Submit:** Your modified script and a log of a conversation showing the agent correctly using the new tool to influence its final answer.

-----
