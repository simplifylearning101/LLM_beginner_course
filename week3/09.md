
### **Week 3: Hour 9 - Optimizing LLM Applications: Cost and Latency**

#### **30% Theory: Beyond Functionality** ðŸ’°

Building a functional LLM application is the first step. Building a production-ready one requires a focus on cost and performance. This hour teaches you how to be a responsible developer by optimizing your applications for real-world usage.

  * **Cost Management:**
      * **Token Economics:** You are charged per token. This includes both the tokens in your prompt and the tokens in the LLM's response. A longer prompt or a longer response costs more.
      * **Strategies:**
          * **Prudent Context:** Keep your conversation history and context as short as possible. Use summarization tools to condense long conversations.
          * **Model Selection:** Use cheaper, smaller models (e.g., GPT-3.5) for simpler tasks like summarization or extraction, and only use more powerful, expensive models (e.g., GPT-4) for complex, high-stakes tasks.
          * **`max_tokens`:** Always set a `max_tokens` limit on your outputs to prevent runaway generation.
          * **Caching:** Store LLM responses to repeated or common queries to avoid paying for them again.
  * **Latency Management:**
      * **The Problem:** LLM API calls are a major source of latency. They can take several seconds to complete, which is a poor user experience.
      * **Strategies:**
          * **Streaming Responses:** Instead of waiting for the full response, stream the output token by token. This gives the user the illusion of a faster response.
          * **Prompt Optimization:** Shorter, clearer prompts often result in faster responses.
          * **Asynchronous Calls:** For applications that require multiple, independent LLM calls, use asynchronous programming to run them in parallel.
          * **Caching:** Similar to cost, caching also improves latency by eliminating the need for an API call.

#### **70% Hands-on Session: Token Counting & Streaming**

  * **Objective:** To implement a token counter to estimate cost and to conceptually understand how response streaming works to improve user experience.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour9_optimization.py`.
          * Copy your LLM setup.
          * Install the `tiktoken` library: `pip install tiktoken`. This library is a popular choice for counting tokens for OpenAI models.

    2.  **Task 1: Implement Token Counting:**

          * **Goal:** Write a function to count tokens in a given prompt and estimate its cost.

          * **Add this code:**

            ```python
            # ... (your existing setup code and get_llm_response_robust function) ...
            import tiktoken

            # --- Task 1: Implement Token Counting ---
            def count_tokens(text: str, model_name: str) -> int:
                """Counts tokens in a string using tiktoken."""
                try:
                    encoding = tiktoken.encoding_for_model(model_name)
                except KeyError:
                    encoding = tiktoken.get_encoding("cl100k_base")
                return len(encoding.encode(text))

            # Example usage
            example_prompt = "Tell me a story about a dragon and a wizard who become friends."
            prompt_tokens = count_tokens(example_prompt, LLM_MODEL)

            print(f"\nExample Prompt: '{example_prompt}'")
            print(f"Estimated prompt tokens: {prompt_tokens}")

            # Conceptual cost calculation (prices can change!)
            # GPT-3.5-turbo pricing: $0.0010 / 1K input tokens
            input_cost = (prompt_tokens / 1000) * 0.0010
            print(f"Estimated input cost: ${input_cost:.5f}")

            # Now, simulate a full LLM call and calculate total cost
            llm_response = get_llm_response_robust([{"role": "user", "content": example_prompt}], LLM_MODEL, max_response_tokens=100)
            response_tokens = count_tokens(llm_response, LLM_MODEL)
            output_cost = (response_tokens / 1000) * 0.0020

            print(f"\nLLM Response: '{llm_response}'")
            print(f"Estimated response tokens: {response_tokens}")
            print(f"Estimated output cost: ${output_cost:.5f}")
            print(f"Total estimated cost: ${input_cost + output_cost:.5f}")
            ```

          * **Run the script.** This will give you a concrete understanding of token cost.

    3.  **Task 2: Conceptual Response Streaming:**

          * **Goal:** Understand how to use the streaming feature of an LLM API to improve perceived latency.

          * **Add this code (this is a conceptual example for OpenAI's streaming):**

            ```python
            # --- Task 2: Conceptual Response Streaming ---
            # NOTE: This code is for demonstration and requires a stream-enabled endpoint

            print("\n--- Conceptual Streaming Demo ---")

            def stream_response_concept(prompt_text):
                # This is what a streaming API call would look like
                # stream = client.chat.completions.create(
                #    model=LLM_MODEL,
                #    messages=[{"role": "user", "content": prompt_text}],
                #    stream=True
                # )
                
                print("Assistant (streaming): ", end="", flush=True)
                # for chunk in stream:
                #    if chunk.choices[0].delta.content is not None:
                #        print(chunk.choices[0].delta.content, end="", flush=True)
                print("The response is now streaming...")
                
            # stream_response_concept("Tell me a long, epic story about the history of the moon.")

            print("\n\nBy streaming, the user doesn't wait for the full response to appear,")
            print("giving the perception of a much faster application.")
            ```

  * **Conclusion:** You now have the knowledge and tools to manage the financial and performance aspects of your LLM applications, which is essential for any real-world deployment.

#### **Homework for Hour 9**

  * **Exercise 1: Optimization Plan.**
      * **Goal:** Create a simple optimization plan for the ultimate RAG agent you built in Hour 8.
      * **Submission:** Write a short document (2-3 paragraphs) that outlines:
        1.  **Cost Reduction:** Two specific strategies you would implement to reduce the agent's cost (e.g., changing models for specific steps, shortening prompts).
        2.  **Latency Improvement:** Two specific strategies you would use to make the agent feel faster to the user (e.g., implementing streaming, optimizing the RAG retrieval).

-----
