
### **Week 3: Hour 3 - Retrieval-Augmented Generation (RAG)**

#### **30% Theory: Context is King**

  * **Objective:** To understand what RAG is, why it's essential for grounded, factual responses, and to learn the key steps involved in a RAG pipeline.

  * **What is RAG?**

      * **Retrieval-Augmented Generation** (RAG) is a powerful technique that allows an LLM to access and reference knowledge outside of its original training data before generating a response.
      * This knowledge can be your company's documents, a database, or any other private information.

  * **RAG vs. Fine-Tuning:**

      * **Fine-Tuning:** **Changes the model's weights.** Used for adapting a model's style, tone, or ability to follow a specific format. It's expensive and can't be updated instantly with new information.
      * **RAG:** **Provides the model with external context at query time.** The model itself doesn't change. It's cheap, easy to update (just add new documents), and perfect for queries that require specific, factual, or private information.

  * **The RAG Pipeline (4 Key Steps):**

    1.  **Load:** Start with your knowledge base (e.g., a PDF, a folder of text files).
    2.  **Embed:** Use an embedding model (a special type of LLM) to convert each piece of text into a numerical vector (a list of numbers). This vector represents the semantic meaning of the text.
    3.  **Store & Index:** Store these vectors in a **vector database** or **vector store** (like ChromaDB or Faiss), which is optimized for fast similarity searches.
    4.  **Retrieve & Augment:** When a user asks a question, your code:
          * Embeds the user's question into a vector.
          * Searches the vector store to find the most similar text vectors from your knowledge base.
          * Retrieves the original text chunks associated with those vectors.
          * Constructs a final prompt for the LLM that includes the original question **and** the retrieved, relevant context.

#### **70% Hands-on Session: Building a Basic RAG System**

  * **Objective:** To build a basic RAG system that answers questions using a small, private knowledge base. We'll use a simple in-memory vector store.

  * **Step-by-Step Instructions:**

    1.  **Preparation:**

          * Create a new file: `week3_hour3_rag.py`.
          * Copy your robust LLM setup from Week 2 (imports, API key, `get_llm_response_robust`).
          * **Install `sentence-transformers`:** In your terminal, run `pip install sentence-transformers`. This library provides a simple way to get a pre-trained embedding model.

    2.  **Task 1: Define Knowledge Base & Embedder:**

          * **Goal:** Create a knowledge base and load a pre-trained embedding model.

          * **Add the following code:**

            ```python
            # ... (your existing setup code and get_llm_response_robust function) ...
            from sentence_transformers import SentenceTransformer
            import numpy as np

            # --- Task 1: Define Knowledge Base & Embedder ---
            # Your private knowledge base (e.g., from a document)
            knowledge_base = [
                "The ApexCorp Q2 report shows a 15% increase in revenue, primarily from the North American market.",
                "Jane Doe is the project lead for the 'Project Phoenix' initiative, which is scheduled to launch in Q4.",
                "The new marketing campaign will focus on social media influencers and a targeted email series.",
                "Budget allocations for the R&D department have been increased by $5 million for the upcoming fiscal year.",
                "Employee benefits include unlimited paid time off, a 401(k) matching program, and a flexible work schedule."
            ]

            # Load a pre-trained embedding model
            print("\n--- Task 1: Loading Embedding Model ---")
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            print("Embedding model loaded. This converts text into vectors.")
            ```

    3.  **Task 2: Embed and Store the Knowledge Base:**

          * **Goal:** Convert the text chunks into vectors and store them in a simple data structure.

          * **Add this code:**

            ```python
            # ... (code from Task 1) ...

            # Create a simple in-memory vector store
            print("\n--- Task 2: Embedding and Storing Knowledge Base ---")
            embeddings = embedding_model.encode(knowledge_base)

            # Store embeddings and original text together
            vector_store = list(zip(knowledge_base, embeddings))
            print(f"Knowledge base with {len(vector_store)} chunks has been embedded.")
            ```

    4.  **Task 3: Retrieve and Augment:**

          * **Goal:** Write a function to search the vector store for the most relevant text and a function to pass that text to the LLM.

          * **Add this code:**

            ```python
            # ... (code from Task 1 and 2) ...

            def retrieve_relevant_context(query, k=1):
                """Finds the top k most relevant documents to a query."""
                query_embedding = embedding_model.encode(query)
                
                # Calculate dot product similarity between query and all stored embeddings
                similarities = np.dot(embeddings, query_embedding)
                
                # Get the indices of the top k most similar documents
                top_k_indices = np.argsort(similarities)[-k:]
                
                retrieved_docs = [knowledge_base[i] for i in top_k_indices]
                return retrieved_docs

            def get_rag_response(query):
                """The main RAG pipeline function."""
                print(f"User Query: '{query}'")
                
                # Step 1: Retrieve relevant context from our knowledge base
                retrieved_context = retrieve_relevant_context(query)
                
                print("Retrieved Context:")
                for doc in retrieved_context:
                    print(f"- {doc}")
                
                # Step 2: Augment the prompt with the retrieved context
                rag_prompt = f"""
                You are a helpful assistant for ApexCorp. Answer the user's question ONLY using the following context.
                If the answer is not in the context, say you don't have that information.
                
                Context:
                ---
                {retrieved_context[0]}
                ---
                
                User Question: {query}
                """
                
                messages = [{"role": "user", "content": rag_prompt}]
                if 'OpenAI' in globals():
                    messages.insert(0, {"role": "system", "content": "You are a factual assistant."})
                
                # Step 3: Get the LLM response
                response = get_llm_response_robust(messages, LLM_MODEL, temp=0.0, max_response_tokens=150)
                return response

            # --- Test the RAG system ---
            print("\n--- Task 3: Testing the RAG Pipeline ---")
            query1 = "What is Project Phoenix about?"
            response1 = get_rag_response(query1)
            print("\nLLM Response:", response1)

            print("\n" + "="*50 + "\n")

            query2 = "What are the key employee benefits?"
            response2 = get_rag_response(query2)
            print("\nLLM Response:", response2)

            print("\n" + "="*50 + "\n")

            query3 = "What is the company's stock price today?"
            response3 = get_rag_response(query3)
            print("\nLLM Response:", response3)
            ```

          * **Run the script.** Observe how the LLM correctly answers the first two questions using the provided context but correctly states it doesn't have the information for the third, demonstrating RAG's ability to stay "grounded."

  * **Conclusion:** You have successfully built a basic RAG system. This is a game-changing skill that allows you to give LLMs access to specific, up-to-date, or proprietary information, making them far more useful and reliable for business applications.

#### **Homework for Hour 3**

  * **Exercise 1: Expand and Test Your RAG System.**
      * Add at least three more paragraphs or sentences to your `knowledge_base` in `week3_hour3_rag.py`. Make some of the new facts related to the old ones and some entirely new.
      * Change the `k` parameter in `retrieve_relevant_context` to `2` to retrieve two of the most relevant documents.
      * Write and test at least three new queries, ensuring they can only be answered by the new information you added.
      * **Submit:** Your updated script and the outputs from your new test queries.